[{"authors":["Alex Hope"],"categories":[],"content":"My experience and thoughts on block 3 of MDS at UBC. The courses were: DSCI 513, DSCI 522, DSCI 561, and DSCI 571.\nIntroduction to Block 3 This post comes to you after finishing block 3 of MDS, the final block of 2018 and the 3rd installment in this blog series. If you haven\u0026rsquo;t already, check out my post about the 2nd block of this degree so far.\nWhat a month it was! I think I speak for everyone in the program and say this was the most intellectually challenging yet gratifying work in the degree thus far. Quite simply, you can start to see the power behind what you are learning in classes like Supervised learning (DSCI 571) and Workflows (DSCI 522) and the importance of having instructors curate content that understand it deeply and know what you need to focus on and what you can learn on your own or ignore. Any student that went through this block can tell you about the basics of linear regression, how relational and semi-structured databases work, how to apply different machine learning algorithms for different problems and the pros/cons of each, as well as how to automate scripts that span the analysis process (wrangling, visualization, analysis and communication). It felt like we were starting to dig into the material that brought me here in the first place, and the content that was the real value I could offer an employer on the other side of this master\u0026rsquo;s program. Without further delay, each class from block 3.\nSupervised Learning (DSCI-571: Dr. Mike Gelbart) Heading into this class I was a bit intimidated looking at the material. Anyone with an interest in data science has heard of sci-kit learn, machine learning, overfitting and underfitting, but I also knew these could be incredibly complex concepts that were not easy to unpack\u0026hellip;. let alone in 4 weeks. To my surprise, I learned more in this class than any other I\u0026rsquo;ve taken in this degree, hands down. Mike Gelbart delivered a \u0026ldquo;flipped\u0026rdquo; classroom style course where before each lecture we were asked to watch a video on a specific machine learning classifier that we would then cover in lecture. The opportunity to absorb information prior to lecture was a huge help in scaling my learning, and allowed me to go deeper into the concepts we covered in lecture as a result. Mike\u0026rsquo;s examples and notes were extremely clear and helpful, and when paired with the labs I could see the power of the sci-kit library and how many of the classifiers could be applied to a range of datasets from music rankings (Spotify), voting patterns in political elections (Democrat vs Republican), to natural language processing (IMDB Movie Reviews), just to name a few. We also covered critical concepts like cross-validation, balancing your training and test error and how determining the \u0026lsquo;accuracy\u0026rsquo; of a given model isn\u0026rsquo;t always as straight-forward as it may seem. Any student leaving this class is capable of: applying machine learning algorithms with the fundamental trade-off of selecting hyperparameters kept in mind, and understanding what algorithms may be best for classification or regression problems.\nThere is so much to learn in this area and while it feels like we\u0026rsquo;ve still scratched the surface on how different algorithms actually work underneath the hood (math and assumptions represented as code within each function) and perform under certain conditions with different kinds of data, it taught me that these topics don\u0026rsquo;t have to be intimidating to get into. In fact, if you focus on a high-level understanding of what\u0026rsquo;s happening when an algorithm is fit to data and predicts on new data and consider WHY some algorithms may be more effective than others at this process, you can walk away with a great foundation of machine learning fundamentals. For anybody whose new to concepts in data science, there can be a sense of fear in students when confronting machine learning courses and understandably so because they are indeed complex topics. While there are a plethora of materials available out there to learn on your own and it has never been easier, what really made MDS worth it is to know what to learn within ML, in what order to best digest the info, and to have someone take you through simple examples of how it can be applied and present important concepts to you in a way that are gradual to help build your intuition and grasp of the information.\nWith that in mind, I have to thank Mike and Varada (Head TA) for designing this curriculum in a way that was extremely digestible. I had heard rumours that DSCI 571 was quite difficult for the previous cohort and I think they both took this to heart in designing the curriculum and labs this year, hats off to them. I also consistently got the impression they cared about the success of students in the course, and would do whatever is needed to help you understand concepts whether its extending or adding office hours, recommending new materials or addressing slack questions promptly.\nFor any future students, coursework like this is likely a core reason why you are applied to MDS, and I can happily say it is also where you will feel like you\u0026rsquo;re growing the most in this degree and I\u0026rsquo;m glad I feel I made the most of this opportunity.\nData Science Workflows (DSCI-522: Dr. Tiffany Timbers) This class was interesting because it was the first of it\u0026rsquo;s kind up until now for this year\u0026rsquo;s MDS cohort. While I had heard issues raised from some students in my cohort about fixed group work for the entirety of the course, I found it to be a great experience with my group partner. This field is an extremely collaborative one, and I know I valued the experience of working through both technical and social roadblocks with a partner. The group work was one unique aspect but it also did not have any exams or labs, only project deadlines each week that asked us to apply what we had learned in lecture regarding script automation and containerization to our project. My partner and I chose an NBA dataset where we applied our supervised learning skills (decision tree classifier) to try and predict whether a shot would be made or missed by Lebron James, and automate our wrangling, exploratory visualization and analysis process. It was an interesting blend of many of the skills we acquired in block 2 regarding visualizations, wrangling as well as the block 3 machine learning skills we cultivated.\nThe real focus of the course wasn\u0026rsquo;t reiterating these skills that we had already acquired though, it was using tools like Make and Docker to automate these processes and encapsulate them in containers that are reproducible across platforms. I intend on applying them to all of my projects in the future since once the framework is built they can deliver faster, more flexible results. For instance, my partner had an ingenius idea to take our project a step further in automating our scripts based on a keyword argument so that when we ran the scripts and provided a \u0026lsquo;player name\u0026rsquo; argument, you could run the full report for any player in the NBA you wanted, not just Lebron James. Now you can wrangle, visualize, and analyze characteristics of shots from any player in the NBA and compute these scripts in under a few seconds. Amazing!\nMy hope is that in the future I can use Make and Docker to build more complex and flexible scripts than the project we built, perhaps a larger analysis pipeline to address questions that interest me and allow me to come back and run new ideas through a pipeline that I think may be interesting. Once the pipeline is built, as long as you are feeding it data that is structured to fit your code, you can generate results in very little time and that seems like a valuable investment for future work with similar datasets and approaches. I highly recommend becoming acquainted with script automation and firmly stand by the belief that it is worth it to invest yourself in this class. If you\u0026rsquo;re working within a larger company when you graduate, there is a very good chance that they will have large pipelines in place already, and you will be asked to take advantage of these in your work. It will save you a ton of time and energy, and you\u0026rsquo;ll be more productive in the long run. So dive in!\nTiffany\u0026rsquo;s teaching style really shines through in courses where there\u0026rsquo;s a lot of in-class coding and actionable instructions to follow in lecture. Her strength as an instructor is taking various packages and platforms that are complex and showing students in a step-wise fashion how the basics actually work. She is great at helping you realize this doesn\u0026rsquo;t have to be so complicated. Every lecture Tiffany also gives us a 5-minute break and plays music, which was an unexpected but greatly appreciated mental break. I know she is mindful of our workload and stress levels, even with the little things like this. Some students had issues with their partner or some aspect of the group work but for myself, it was a great experience where I could learn lots from both Tiffany and my partner but also not sweat looming deadlines of labs or exams for the class. This helped with balancing labs and exams for the other classes as well.\nFor anyone interested in the NBA project my partner and I worked on for this class, it can be found on GitHub here: Lebron James for T(h)ree!\nRegression I (DSCI-561: Dr. Gabriela Cohen-Frue) Heading into this class, I knew exactly what to expect and that it wouldn\u0026rsquo;t be easy for me. I had encountered linear regression in one of my past positions in mental health research, but admittedly, did not fully understand the ins-and-outs of what I was assuming and had generally struggled through statistics classes in my undergrad. Undoubtedly, I worked the hardest in this class to understand the material, but I can only imagine how much harder it would have been if Gabriela had not been our instructor. I thought this class moved at a reasonably slow pace, where we really focused on the core components of simple and multiple linear regression across 7 of the 8 weeks. Gabriela re-iterated many themes in this class at both the theoretical level with respect to the thinking behind concepts such as cell-means versus reference-treatment parameterization, errors vs residuals and least squares fitting, but also how this was expressed in R\u0026rsquo;s output.\nUltimately as data scientists we will be working with a programming language that computes many of these statistical measures, and so the burden of understanding falls more on the side of using R\u0026rsquo;s functions and interpreting their output than explicitly calculating the measures and remembering formulas. I thought in general she balanced these demands quite well, but think she could stray a bit further from the calculus aspects of how certain measures are calculated and get back to R. I really wanted a deeper discussion of the direct R output and the higher-level forms of the concepts we were discussing like multicollinearity, and the coefficient of determination, among others. With only 4 weeks to learn regression spending a lecture or two on the calculus behind formulas simply isn\u0026rsquo;t productive in my opinion. However, in general she is a wonderful instructor with an intimate understanding of statistics that can\u0026rsquo;t be understated. Ask a ton of questions in this class, I promise you\u0026rsquo;re not alone in wondering what that standard error formula means, how to manage outliers, or different forms of fitting regression models. This is no fault of the instructor, statistics is a slippery fish of knowledge that takes a lot of effort and attention to not only understand but apply properly. The more questions you ask and the more you can remain patient in your learning, the firmer the grasp you will have on this statistical fish.\nI thought Vincenzo (TA) did a solid job with the lab design as well, gradually building our intuition of how regression truly worked while challenging us to consider what the output of lm vs anova vs aov really meant. Overall, this course was a pleasant surprise given my past struggles in statistics. Gabriela and Vincenzo will help you see the value in understanding these concepts as they will undoubtedly arise in an analytics career. Examining relationships between data, fitting linear models and making predictions about future observations are all critical skills in an analysts toolbox, choose to cultivate them!\nDatabases and Data Retrieval (DSCI-513: Bhav Dhillon) Heading into this class I had no experience with SQL or really any relational or semi-structured databases and while I did finish with a reasonable understanding of the materials, this class left a fair bit of room for improvement. A common trend I am noticing within the degree is that the classes with instructors who are furthest removed from the computer science/MDS/statistics faculty tend to be the worst from an educational standpoint. I don\u0026rsquo;t necessarily say this with respect to their ability as an instructor because I don\u0026rsquo;t think that\u0026rsquo;s the case at all, in fact they have been very talented and driven instructors. What I mean is it\u0026rsquo;s harder to integrate material from the lecture into the labs and draw from past successes in other classes that may be useful for the instructor if they communicate less with the core faculty that curates the overall curriculum. This gap in communication reflected itself in general student engagement, and concept acquisition in DSCI 513. The instructor was a post-doc brought in to teach DSCI-513 and who was not embedded within the overarching educational picture of the program and was not familiar with the content we had already learned that may inform their teaching (ex. inner joins, select, etc.). Bhav was brought in to teach this specific class and the lectures were quite poorly laid out with respect to how they integrated with labwork, plain and simple. That said, she was kind, knowledgeable and always available for help and was much more effective one on one with students I found. Again, I don\u0026rsquo;t think it\u0026rsquo;s a reflection of her abilities at all, simply an organizational critique. The overarching concepts were laid out by Bhav and while the early lectures seemed to confuse her at times along with the students, by the end they were clear and more useful given that we hadn\u0026rsquo;t encountered the later concepts in previous courses.\nEvery student can tell you about primary and foreign keys in databases, and ER diagrams for different forms of relationships between information. That said, I can\u0026rsquo;t help but feel like this class still left lots to be desired. It should have been what data wrangling (DSCI-523) was but with SQL/JSON and it just didn\u0026rsquo;t live up to that standard. An additional issue was having to use SQLite, which is not really a common language in the working world despite it\u0026rsquo;s syntactical similarities to MySQL, the more common language. Again, the reason for this is UBC does not have an infrastructure in place for relational databases so we were forced to use SQLite. In my opinion, organization would go a long way in improving this class not necessarily changing the instructor or the material itself per se.\nWithout being a huge downer, I still learned lots from this class, and Rodolfo (TA) challenged us in the labs which forced me to get creative with my learning. The majority of my breakthroughs in lab work came through trial-and-error and referencing alternative resources beyond lecture notes to develop the necessary understanding to get through difficult questions. Having to go that extra mile elevated my thinking around how to work with databases, the best ways to design them given particular constraints, and I improved drastically at visualizing different kinds of joins between separate tables! So I guess I have that going for me. In sum, I hope for next year they find ways to improve upon this course structurally, and increase the communication between core faculty and the course instructor regarding what concepts we may have already covered. I still was able to learn a ton about databases, it just took more energy given the constraints I\u0026rsquo;ve mentioned but if you\u0026rsquo;re motivated, you\u0026rsquo;ll be just fine.\nClosing Thoughts \u0026amp; Suggestions Block 3 was by far the most action packed block regarding the density of material and the most exciting given the power of what we were learning. Generally speaking I have great things to say about the teaching staff in this block, I know far more than I did a month ago and don\u0026rsquo;t see myself forgetting the core materials in each class because they were so fundamental. Linear models, machine learning algorithms for classification and regression, how relational databases work via keys and constraints, and script automation are such valuable bodies of knowledge for a data scientist, analyst or really anyone working with data. Aside from some structural issues with one course and the recurring fact that I just don\u0026rsquo;t have enough time to go into everything that intrigues me, this was a great 4 weeks.\nAdditional Ideas/Recommendations One idea that came to me while working in this block is for the wrangling and database classes or really any class that requires more technical skill is to provide a \u0026lsquo;puzzle dataset\u0026rsquo; to students at the start of the block that has various manipulations and is considered \u0026lsquo;untidy\u0026rsquo;. As students learn the skills for a course (perhaps multiple courses) they will be able to apply it to this puzzle dataset and at the end possess all the skills to actually solve it. They can then reference a key that the TA has, and it provides some optional work for students to get practice and tackle optional problems. Obvious classes where this can be implemented are 523 with R (dplyr) and 513 (SQL) but any course where instructors feel they can offer a big picture assignment that students can tackle might be a nice addition to the regular lab work. This could be extended to questions where a dataset must be wrangled and a linear model must be fitted to some portion of the data once it is tidied, incorporating multiple classes and areas of understanding to resolve the problem. It also comes at very little cost to the TA\u0026rsquo;s because the head TA is designing databases to be worked with already by the students, and provides one more large project the students can grade themselves by comparing with a key.\n","date":1545727981,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545727981,"objectID":"14af76675ac9a2acf1f0eea4aba33427","permalink":"https://ehhope.github.io/post/mds-block-3/","publishdate":"2018-12-25T01:53:01-07:00","relpermalink":"/post/mds-block-3/","section":"post","summary":"My experience and thoughts on block 3 of MDS at UBC. The courses were: DSCI 513, DSCI 522, DSCI 561, and DSCI 571.\n","tags":["Machine Learning","Regression","SQL","Script Automation","Docker","MDS","UBC"],"title":"MDS Block 3","type":"post"},{"authors":["Alex Hope"],"categories":[],"content":"My experience and thoughts on block 2 of MDS at UBC. The courses were: DSCI 512, DSCI 523, DSCI 531, and DSCI 552.\nIntroduction to Block 2 I want to break down my thoughts for this block course by course because I have a lot to say, and it doesn\u0026rsquo;t neatly cut across topics, not to mention, my experiences with courses in this block were quite different. First, a few general thoughts before I dive in. I got the most out of courses that I invested myself in, plain and simple. While that seems like a common trope, it really holds true in data wrangling, and algorithms \u0026amp; data structures where the degree of difficulty is quite high in the content, and the lecturer is forced to feed you complex material quickly given the time constraints. The content in a couple courses was quite difficult, and I found myself scrambling two of the four weeks to get my labs in despite putting a ton of effort into my labs. Learning these skills and forms of thinking requires considerable persistence, and for that reason I was ok with barely finishing. This material is supposed to be difficult! That\u0026rsquo;s the point! So with that expressed let me talk about each class.\nAlgorithms and Data Structures (DSCI-512: Patrice Belleville) This was undoubtedly the most difficult course of the block, and the concepts were at times challenging to grasp. The reason they were hard to follow was primarily because of the speed of the curriculum where we would cover hash tables in 25% of one lecture, and touch on different forms of search trees for the rest of lecture before diving into a single lecture on various kinds of directed and undirected graphs. The material was complex but still digestible, and Patrice was a strong instructor, but the time constraints placed a significant burden on people\u0026rsquo;s learning where there was a tendency to learn to get through the course rather than learn to absorb the material of the course for future reference. When the material scales up significantly in difficulty, this was a common feeling in students and I certainly felt it at times as well. For perspective on the pace, undergraduates will spend an entire semester or two examining the full range of material we covered in a single month. This is not only a huge challenge for students, but also the instructor to curate the material and emphasize what is only crucial while discarding or grazing briefly on an area.\nIn addition to the pace, there was a huge gap between the lecture material and the questions being asked in labs where the content we covered needed to be implemented but in a way that was far beyond the coding ability of most of the cohort. Given the general class response I knew I wasn\u0026rsquo;t alone in struggling to apply my understanding of recursion to sierpinski\u0026rsquo;s triangle, or image resizing, but I also secretly enjoyed confronting this massive challenge. This was not an easy introduction to algorithms and how they are implemented in data structures, and I need to learn more on these topics after finishing this class to round out my learning given their importance in data science, but I felt it was more of a structural issue with the disconnect between lecture material and lab material than Patrice\u0026rsquo;s poor teaching that really left me with this impression.\nTo excel in this class you need a strong familiarity with object oriented programming, and should possess an aptitude for reading code that is likely more complex than you can write at this stage. I saw this class as a tough challenge that elevated my thinking on the topic, but didn\u0026rsquo;t provide enough scaffolding to really help me understand the mechanics of how every algorithm worked or how they were implemented in different problems in the labs such as with facebook data that was provided in one lab. That said, if you listen in class you will walk away with an understanding of breadth-first vs depth-first search trees and how they are represented in python. And the different forms of searching whether with rote methods or through recursion. In addition, we spent considerable time looking at how different kinds of graphs can represent information about individual entities, as well as the relationships between these entities. In some cases they were bi-directional and others they were uni-directional depending on the flow of information. The big picture here was, if we wanted to model an algorithm after a particular social media site, or scheduling problem, what would we choose? This high-level planning requires you to know the details about how information is exchanged, referenced and what graphs will and will not allow as far as the flow of that data.\nThis class really could\u0026rsquo;ve been more effective if it had students with a higher coding literacy, and I would beg the instructors to consider scaling up the intro coding classes in block 1 to cover OOP in greater depth and have students write functions sooner in DSCI 511 so they can approach difficult questions in labs with greater confidence. Having to learn to code on top of understanding the algorithms can be daunting, and it would serve all parties involved to make this change I think.\nWhile it seems like I have a lot to say that\u0026rsquo;s negative, part of being a student is also embracing challenge and uncertainty. I learned a lot in this class because I felt like I had to in order to do well, and that pushed me to invest more of myself in it than other classes where I felt it was easier to grab the content quickly and implement the knowledge. Here, I had no choice but to study hard, and that\u0026rsquo;s what being a graduate student is all about\u0026hellip;.. digging deeper.\nData Wrangling (DSCI-523: Jenny Bryant) Data wrangling is a fundamental skill in data science, and that is a fact. A significant portion of your time as an analyst will be spent simply preparing a dataset for analysis and deep exploration. Whether it\u0026rsquo;s changing the type of data in columns, creating 2 columns out of information contained in a single column, creating columns that are aggregates of other columns, joining different tables together, or simply grouping variables together to extract meaning when grouped by their factor, data wrangling really matters and it won\u0026rsquo;t take you long to see why.\nI enjoyed this class the most of any course in the block and perhaps to date in this degree. It was difficult and frustrating at times banging my head against a problem that required multiple joins or a function I wasn\u0026rsquo;t used to applying like lag. Let\u0026rsquo;s just say I had to take many breaks from the computer to finish these labs but out of this immense frustration was an extreme sense of reward when I would learn to parse data in that column, or adjust the time with respect to timezone, or just manipulate data how I intended. I felt powerful, and these are pivotal skills you need to know.\nHaving no experience in R prior to this degree, I couldn\u0026rsquo;t imagine a smoother experience wrangling than with the Tidyverse. It\u0026rsquo;s reputation exists for a reason, it\u0026rsquo;s straight forward and sensical to beginner wranglers and our instructor (Jenny Bryant) is the maintainer of this entire package. Most of all, I was really learning a tangible skill that is readily applicable to personal projects and were employable skills. Along with visualization, it instilled a feeling that I could start to hit the ground running on projects and make reasonable progress working with any data I encountered. The course also provided immediate feedback to me on my abilities and I found that to be quite reinforcing in a way that a class like probability or algorithms was not.\nAs far as Jenny Bryant, her reputation also speaks for itself. She is a world class instructor with quality lecture materials, resources and a great sense of humor that kept me connected to the class and her teaching. I know the cohort genuinely found her lectures to be intriguing, helpful, and balanced in their difficulty as well, introducing us gradually to more and more complex wrangling techniques. It was a pleasure to have her for this class and reassuring that someone heavily involved in the maintenance of the Tidyverse was teaching us how to use it.\nYou may be thinking well wait\u0026hellip;. what about Python? We covered Python\u0026rsquo;s Numpy and Pandas in one class taught by Mike Gelbart, which in my opinion deserved a second lecture given how much Python will be used in ML blocks coming up but mainly focused on R in this class. I believe the thinking was that we can always wrangle in R prior to analyzing in python and while that is fair, python is such a prominent language it would be nice to attend to it\u0026rsquo;s wrangling libraries as well. Admittedly, I didn\u0026rsquo;t really learn anything new in this lecture simply because I knew the Pandas library quite well already. Nonetheless, I am much more confident in my wrangling abilities and genuinely enjoy the challenge of reshaping datasets.\nFor anyone taking this class, you will be well acquainted with how to join different datasets based on particular keys and the type of join you want, how to select particular information in a dataframe, what a tibble is compared to a dataframe, how to add columns, delete data and replace NA\u0026rsquo;s. All of these things are important skills to possess, and I felt for the month we spent wrangling, we covered this material very well. I banged my head on the practice problems enough that the material was seamlessly drilled into my head, and it was so worth it. These are skills anybody working with data will continue to use in the future.\nVisualization I (DSCI 531: Vincenzo Coia) In general, I thought Vincenzo did a great job with this class. He took us through basic graph theory as far as what various plots convey in their variables and information, but also gradually taught us the relevant syntax for ggplot, which has become a personal favourite for visualizing data. Also, graphing data to gather a sense of patterns is under appreciated and not talked about enough. A common mistake is to want to start analyzing immediately and rely on visualizations for the final results to convey information, but Vincenzo really taught me a lot about patience and understanding data through graphs PRIOR to starting any analyses. Get to know what you are working with before investing your time with analysis! Similar to wrangling, we were only introduced to Matplotlib (Vincenzo) in one lecture and Seaborn (Chris- TA) in another lecture. I got a lot out of the Seaborn lecture from Chris, and was relieved to see visualizations in Python can be as easy as in R, but would have loved to see a bit more with respect to python here to round out the class.\nOverall, Vincenzo is a fantastic post-doc who is extremely knowledgeable in R and can convey the basics of graphing extremely well. The only recommendation I have for him as an instructor is to consider a lecture through the lens of a new analyst that wants to understand relationships in the data they have. How can they go about this graphically? What should they look for in the data, what plots should they try, how many different graphs should they plot to get a sense of any patterns? Some form of exploratory thinking would go a long way in adding substance to the syntax we learned and provide much needed direction to students who lack this internal compass. Other than that though he was a good lecturer and whether you have the intuition to explore data or not, anyone who took this class can write the syntax to accomplish what they need.\nYou\u0026rsquo;ll know what kinds of information bar graphs, scatterplots, pie graphs, box plots, word clouds, heatmaps, and map graphs represent and what variables are required to create these visualizations. Also, you\u0026rsquo;ll learn about the ingredients of graphs, so what is the x and y-axis? Are the scales on a log, or normal axis? Can you facet by a certain group to convey multiple plots within a single graph that are separated by group? These sorts of considerations are all touched on intelligently in lecture, and you\u0026rsquo;ll feel comfortable graphing anything basic that you need by the end. Looking forward to seeing what Data Visualization II has in store for us.\nStatistical Inference I (DSCI 552: Tiffany Timbers) First, Tiffany teaches clearly and crisply with great lecture examples that offer insight into her labs directly, so attending her lectures is informative for the work you\u0026rsquo;ll be doing outside of them. She\u0026rsquo;s also a dynamic instructor that educates with class coding, group discussion, and through classic speech and it definitely helped with holding my attention on a topic that doesn\u0026rsquo;t exactly grab most people\u0026rsquo;s focus at first glance.\nBootstrapping is a powerful method for inferring about the meaning of a sample by generating thousands of samples and comparing your original sample test statistic with ones generated from other samples that were bootstrapped. When you do this you get a distribution of test statistics that resembles a normal distribution. This is how you can begin to compare whether there is anything special with your sample in comparison to the many others you\u0026rsquo;ve generated.\nWhile the relevant test statistic you are calculating and assumptions made with a t-distribution (CLT) was nothing new, I still learned the syntax for t-tests, confidence intervals and p-values through R\u0026rsquo;s infer package. It was nice to run over basic stats with Tiffany, and learn about applying bootstrapping as a technique in R, but I also recognize the need for more practice to hammer in these concepts on real live datasets where I\u0026rsquo;m generating my own null/alternative hypotheses and running t-tests and anova\u0026rsquo;s. R is such a powerful statistical language and can do so much of the heavy lifting for you, just ensure you know what you\u0026rsquo;re entering for inputs and how to read the output of your functions.\nAny student that finished this class can tell you what a p-value is and what it signifies with respect to a z-score or t-value in the context of a normal distribution, and what assumptions about normality and standard error exist. Overall, this was a solid class that was a helpful review and introduced us to how R can make our lives infinitely easier once we understand the theory.\nClosing Thoughts on Block 2 My overall impression was positive here, there were far more great moments where I felt absorbed by what I was learning than awful ones where I\u0026rsquo;m overwhelmed but I\u0026rsquo;d be lying if I said this was an easy block. The feeling of drowning in information is part of diving into a new area, let alone one that is extremely complex, new as a field, and constantly evolving like data science. Block 2 definitely felt like a deeper dive into new territory and you can tell the material has started to take an upward incline in difficulty but I felt like I rose to the occasion and as the difficulty increases you can really start to see the value in what you\u0026rsquo;re learning and how it can be applied. This is itself quite motivating, and definitely helps you see the finish line as far as the skills you\u0026rsquo;ll possess and will be required of you beyond this degree. I grew so much with R in this block, despite having serious doubts about it as a language that I would enjoy using in my work. Now, infer, ggplot, and dplyr have completely changed my mind and I will happily use the Tidyverse for future work.\nAt a broader level, the terrain we covered feels like the elementary, foundational skills for data science, and looking at the block 3 course curriculum I\u0026rsquo;ll be curious to see how things progress to the higher-level concepts with regression and machine learning.\nOne major piece that continues to stick with me is the timing of this degree. If this degree was even 6 weeks longer and the curriculum was the same, I think everything would feel a bit less rushed and I\u0026rsquo;d have the time to absorb aa bit more deeply. Don\u0026rsquo;t get me wrong, I am learning a ton, but anytime you try and integrate as much info as this degree demands, you are bound to forget things along the way if it is not used. It\u0026rsquo;s an inevitable downside of the human brain, but that said I\u0026rsquo;m glad I\u0026rsquo;ve squeezed what I could and as it stands I know the material extremely well and will continue to as long as I use it.\nWhat a month it was and onto the next block we go! Until next time\u0026hellip;.\n","date":1545727404,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545727404,"objectID":"72a2d3319581ac103a56d2ace808162a","permalink":"https://ehhope.github.io/post/mds-block-2/","publishdate":"2018-12-25T01:43:24-07:00","relpermalink":"/post/mds-block-2/","section":"post","summary":"My experience and thoughts on block 2 of MDS at UBC. The courses were: DSCI 512, DSCI 523, DSCI 531, and DSCI 552.\n","tags":["UBC","MDS","Data Science","Grad School"],"title":"MDS Block 2","type":"post"},{"authors":null,"categories":null,"content":"","date":1545560112,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545560112,"objectID":"b8b5fbbf5d97e48f6720e5344068cac1","permalink":"https://ehhope.github.io/project/brief-intervention/","publishdate":"2018-12-23T03:15:12-07:00","relpermalink":"/project/brief-intervention/","section":"project","summary":"An ecological assessment tool I developed to track anxiety \u0026 depression symptoms \u0026 introduce mindfulness-based techniques to patients enrolled in a BRIEF clinical study.","tags":["intervention","ecological assessments","e-health"],"title":"BRIEF Intervention","type":"project"},{"authors":null,"categories":null,"content":" Decision Tree Analysis of Makes \u0026amp; Misses in the NBA A case study: Lebron James This project focuses on the 2014-2015 NBA season with a dataset containing characteristics of every shot (shot distance, defender, shot clock, etc.) taken by every player in the NBA. The approach used a decision tree classifier to predict whether a shot was made or missed based on shot features for each shot taken by the player. Lebron James season with the Cleveland Cavaliers was used as a case example, however, the script is flexible enough to wrangle, visualize with several plots, analyze and report data for any player if specified as an argument when using make.\nOur decision tree classifier predicted Lebron James at a marginal ~65% when hyperparameters were optimized, however, other players we were able to score upwards of 85%. The reason for the model\u0026rsquo;s poor accuracy with Lebron James, Steph Curry, and other top shooters is that the features don\u0026rsquo;t segregate makes/misses as cleanly as players who are reliably poor shooters based on changes in particular features such as shot distance. An NBA center will struggle to shoot long range shots, therefore when shot distance increases, our classifier is quite accurate in predicting the outcome.\nThis project is also reproducible through docker. Inside the repository linked below, you will find a dockerfile with instructions for running all of the scripts with their respective dependencies (R and python libraries). If you don\u0026rsquo;t have all of the dependencies installed locally that\u0026rsquo;s ok, just use the dockerfile written to run the scripts in the container provided.\nFor easily reproducing and generating data regarding NBA players, this project uses make to run multiple scripts in terminal as well. If interested, you can find the repository on my github account here with instructions in the README file for using these tools.\n","date":1545553574,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545553574,"objectID":"64ac8478f247230d616bd2af4a89e951","permalink":"https://ehhope.github.io/project/nba-shot-decision-tree/","publishdate":"2018-12-23T01:26:14-07:00","relpermalink":"/project/nba-shot-decision-tree/","section":"project","summary":"A shot analysis of NBA players using a decision tree classifier of makes/misses from the 2014-15 season.","tags":["supervised learning","NBA","Lebron James","decision tree"],"title":"NBA Shot Classifier","type":"project"},{"authors":[],"categories":[],"content":"My experience and thoughts on block 1 of MDS at UBC. The courses were: DSCI 511, DSCI 521, DSCI 542, and DSCI 551.\nHey! My name is Alex Hope, if you\u0026rsquo;ve made it this far I\u0026rsquo;ll spare you the full introduction, however, this post is the first of an ongoing series I\u0026rsquo;ll be doing as I progress through my Master\u0026rsquo;s degree in Data Science (MDS) at the University of British Columbia (UBC). There\u0026rsquo;s a bunch of reasons why I decided to write this blog, but a major one was to track my own growth as I transition into this field from healthcare research so I can look back on how far I\u0026rsquo;ve come and recall concepts from the coursework of MDS in the process. I\u0026rsquo;m excited to dive into this new degree and learn more about the analytical and computational skills I knew were important but never had the time to cultivate while working. In addition, I wanted to use this blog as a resource for future students in MDS who like myself may have been unsure about what to expect as a student in MDS, so I hope any of this may be helpful. This blog is a perspective from someone who is simply going through the program as a student, not as a vested employee of UBC. So, you\u0026rsquo;ll hear my thoughts and advice about courses and instructors and I will do my best to convey my feelings about each class as honestly as I can. First a bit of background about what I did prior to getting here.\nThe resources I used to prepare for this degree: * Work experience writing code to do basic analysis in Python and building a basic site with HTML/CSS * Jose Portilla\u0026rsquo;s: Python for Data Science, and Complete Python Programming Bootcamp * ProjectEuler.com - bunch of coding problems on there * Probability and Statistics materials from a one day trial of a bootcamp called \u0026ldquo;Metis\u0026rdquo; * DataCamp: 4 courses (python and basic statistics)\nBlock 1 With that in mind lets kick this off! My cohort contained about 70 students with people of all ages, genders, and countries with diverse skill sets. We have former business owners, rehabilitative therapists, engineers, mathematicians, fellow neuroscientists, programmers, even a practicing doctor. As far as the coursework, there are 6 blocks in MDS with each lasting ~ 1 month. Each block contains 4 classes, the first four being: DSCI 511 (Intro to Programming), DSCI 521 (Computing Platforms for Data Science), DSCI 542 (Communication \u0026amp; Argumentation), \u0026amp; DSCI 551 (Descriptive Statistics \u0026amp; Probability). Essentially, every class has a lab due on the Saturday (4 labs due each week), with midterms taking place during the 2nd week of the block and then finals being the 4th week of the block. So it\u0026rsquo;s pretty fast-paced. Without further a due, let\u0026rsquo;s dive in.\nIntroduction to Programming (DSCI 511) This class was an interesting split between R and Python, with separate instructors that taught us for each language. Heading into the fall I knew I needed to sharpen my programming skills to be able to hit the ground running in this degree and the last thing I wanted was to fall behind early on. Having used Python in my past work, I wasn\u0026rsquo;t worried about keeping up for this part of the class but was concerned about the fact I had zero experience using R. I had heard that R was this language of the past and that it would be overtaken by languages like Python and Julia eventually, so I chose to focus on Python than splitting my learning between languages. In truth, I loved learning about R, and found the in-class tutorials by Vincenzo Coia to be extremely helpful and easy to grasp. Now that I\u0026rsquo;ve used R I feel like it\u0026rsquo;s arguably an even easier language to learn and use immediately than Python, and can see why it\u0026rsquo;s recommended as a great starting language. On the python side, I would say I over-prepared for this class, I wasn\u0026rsquo;t challenged much to be honest. If I had looked at even a little R I think I would\u0026rsquo;ve been fine getting through this block 1 class. I enjoyed the in-class programming tutorials that linked the ideas they were trying to convey with motor-learning, it always helps to write the code yourself and follow along I find, and both instructors took their time conveying the material while providing notes for you to refer back to if needed.\nIn general, the class lectures were well paced with more difficult labs than exams, and they truly distill the basics of what is required to get comfortable writing functions, understanding object-oriented programming (OOP), lists, vectors, dictionaries, and using different kinds of loops, which I trust we will be deploying on an everyday basis in future blocks. There were students that barely used either language or had zero programming experience heading into the fall who did fine, so while the requirements suggest you need to have all of these mathematical, statistical and programming skills to be ready for the degree, I wouldn\u0026rsquo;t say it\u0026rsquo;s absolutely necessary, but it definitely won\u0026rsquo;t hurt. I\u0026rsquo;ll be curious to see how they emphasize these languages differently as we progress to different parts of the data science process (graphs, analysis, reports, slides, etc.) where perhaps having a greater proficiency will help. As far as getting through this class though and performing well, you don\u0026rsquo;t need much of a background honestly.\nComputing Platforms in Data Science (DSCI 521) I wasn\u0026rsquo;t quite sure what to expect from this class heading in as I had never really used any platforms aside from Jupyter that were for data science per se (SPSS and Tableau aren\u0026rsquo;t really considered DS platforms in my opinion). Tiffany Timbers taught this class and introduced us to the beauty of GitHub, RStudio (Markdown and RMarkdown) and Jupyter Notebooks (RISE).\nGitHub is a well-known service that allows people to share all kinds of projects, tools, code, webpages, among other things seamlessly to the web and between people. It\u0026rsquo;s a big part of advertising your own work as well as collaborating with others on things so I\u0026rsquo;m glad we focused on it because it\u0026rsquo;s a great way to gain traction with employers to show them what you can do. On a practical note, we will be using it across all of our classes throughout the degree to pull labs and quizzes from the cloud to our local machine and I suspect we will be cloning, adding, committing, pulling, pushing and forking a lot over the next year\u0026hellip;\nHaving learned about RStudio in DSCI 511, it wasn\u0026rsquo;t completely foreign to me, but there is lots to learn that you simply can\u0026rsquo;t cover in one class as well, so I\u0026rsquo;m glad we covered it in two classes. One aspect I did learn was how to change the output of md and Rmd files to knit different sorts of files into slides, pdf\u0026rsquo;s, or any other useful file imaginable! In addition to RStudio is Jupyter Notebook, which is a complimentary tool used to write code, as well as write documents and will be a staple within the program\u0026rsquo;s learning technologies. I had used Jupyter in the past while learning Python so I was familiar with how it worked in some ways, but I was unaware of RISE which is a way to use notebooks as a powerpoint presentation but with additional features that allow you to have interactive code embedded in your presentations. Very cool.\nIn general, this DS stack seems useful across personal and business settings and I could see myself continuing to use all of these in the future for presentations, writing, sharing documents and coding. Tiffany was great at conveying the basics, showing us directly how they work without overwhelming students and providing resources to take our learning even further beyond lecture if they wanted. She is also extremely responsive on slack, even at odd hours of the day when you think she would be sleeping or doing other human things on a Sunday, she will often respond. Her commitment to students in this class was pretty hard to miss.\nI didn\u0026rsquo;t find the material overly challenging, and really you will learn to use these technologies simply through sheer practice because you will lean on them heavily across almost all of your classes in this block and future ones. Tiffany also demonstrated how to host your own website through GitHub Pages (which you can use to show off your MDS work), how to fork other people\u0026rsquo;s repositories that provide the framework for writing a book, and how to post issues and comments to other repositories as a means of helping correct for bugs. There is so much to explore within GitHub.\nOne day I will write that bio technology book\u0026hellip; one day. No time right now in MDS\u0026hellip; but one day.\nCommunication and Argumentation (DSCI 542) David Laing (a former MDS student) was the instructor for this class, which was the only one of it\u0026rsquo;s kind that you wouldn\u0026rsquo;t consider \u0026lsquo;technical\u0026rsquo; and I have to say I really enjoyed the material. The class focused on \u0026lsquo;softer skills\u0026rsquo; in data science around how to frame problems, communicate the meaning of statistical results and build awareness for how complicated it can be to communicate ideas to different audiences, and I thought David\u0026rsquo;s strong organization and focus on communicative clarity in lectures helped with digesting the material and got you thinking. He also provided ample time to discuss with classmates problems he had framed, and asked us to participate in daily exercises that incorporated the material he was teaching. Overall, he did a great job.\nA common saying hammered into me from this class that was handed down from Hadley Wickham - \u0026ldquo;it doesn\u0026rsquo;t matter how good your analysis is unless you can explain it to others: you need to communicate your results\u0026rdquo;. Start to finish communication is an important aspect of data science and the more I progressed through this class the more I realized the truth behind this fact. From the beginning to the end of a data science project, communication is an important component and can save enormous amounts of time and energy when done well, but haunt you for hours/days/weeks on end when done poorly. Writing code and planning the overarching pieces of an individual project can be challenging because you aren\u0026rsquo;t always clear with yourself on what\u0026rsquo;s required let alone when you have to work with others on a large project.\nWe spent time talking about the barriers to communication, mainly, struggling to imagine how to explain a concept to someone who doesn\u0026rsquo;t understand data science or does but at a different level than you. This includes the tendency to use jargon you comprehend that others don\u0026rsquo;t, employ complex examples and make assumptions in your writing that may need to be spelled out. Understanding the literacy of your audience and how to present information in a way that makes it easy for people to digest is a crucial skill that I\u0026rsquo;m glad we could learn to harness here. One concept was to use bottom-up perspectives where examples are presented to convey the critical point of your message in a simple way before elaborating further on the topic once a case has been presented to the listener.\nIn the end, each student was asked to write a blog post on any topic they wanted in Data Science that attempted to capitalize on the techniques we had learned for presenting information clearly. I thought it was an excellent exercise that summarized the course well, and gave us something to contribute to our webpage that we had developed in DSCI 521 (you can see how these courses start to gel together). At the end of the course, we gave a presentation on our blog post or a data science problem we had formulated from a later lab. I thought the public component really forced students to integrate the knowledge from lecture, and you could tell MDS was organized because they recommended that we use RStudio or RISE to present our topic to the class to further consolidate what we were learning.\nI really enjoyed this class and it was a nice twist from the technical classes we had been focused on. I thought David was fantastic and hope he continues to teach as you can tell her clearly enjoys it. At worst David got you thinking about the challenges of explaining data science, and at best he changed your thinking, writing and speaking so that you delivered content in a more conscientious way to the end consumer, whoever that may be.\nDescriptive Statistics and Probability (DSCI 551) This was probably the one class that you would like to have had some background in coming into MDS block 1. As someone who always struggled with statistics in undergrad, it would\u0026rsquo;ve been nice to look at some basic stats \u0026amp; probability a bit more than I did. The class isn\u0026rsquo;t hard if you invest yourself in it and listen closely in lecture, but make no mistake it was the toughest class of block 1 for English speaking students (DSCI 542: Communication was supposedly tougher for many ESL students).\nMike really focused on descriptive stats (variance, standard deviation, entropy, mean, median, and mode), probability (conditional, joint, and marginal distributions) and examining what different kinds of distributions look like using graphs. As a data scientist, probability is a fundamental knowledge base for all sorts of ML and statistical procedures (Ex. Bayesian Theory) and I knew this was pivotal for what we would learn moving forward in MDS.\nI thought Mike Gelbart (professor) did a pretty good job educating us on how distributions work (Bernoulli, Gaussian, Multinomial), and showing us how R can help derive different measures of distributions using it\u0026rsquo;s functionality as well as emphasizing theory. He is also extremely responsive in the slack channel if you have any questions or hit a snag. Similar to all of the professors, the notes were in Jupyter notebooks and I found playing around with the interactive notebooks for this class to be helpful for learning about the behavior of distributions. Mike is very good at explaining concepts simply, and presenting it with multiple perspectives which I appreciated since this class was by far the least intuitive to me. I left with a strong grasp of parameters for distributions, visualizing conditional and joint distributions, correlations in graphs, and how differences in variance and entropy can effect the shape of distributions when graphed. I had to work the hardest in this class but like anything, you get what you invest into these things.\nTo conclude, this was a great first block and the organization between courses in how they integrate is unparalleled. It\u0026rsquo;s a huge help to have people curate material in a way that\u0026rsquo;s intelligent and efficient, and you truly do get the impression this has been well thought out since the program\u0026rsquo;s first inception. The effort MDS faculty have put into designing the program does offload some of the burden of having to \u0026lsquo;know all the prerequisites\u0026rsquo; before you start your degree despite MDS being a fast-paced learning environment, make no mistake.\nIn my opinion, students who feel they aren\u0026rsquo;t prepared will most likely be just fine for this intro block. Of course, the more you know going in the more free time you have to focus on side projects and dig deeper into material you know the basics of already so there is that. I can tell there\u0026rsquo;s a lot of talented, driven people to work with this coming year, which is exciting.\nOne last point that I think is important is the program encourages us to learn as much as we can by specifically NOT breeding a sense of competition amongst peers. You can see this in how they have us interact during lecture, troubleshoot each other\u0026rsquo;s problems on MDS\u0026rsquo; slack, and emphasize that our classes are not curved. It helps breed an awesome collaborative spirit within the cohort that will resonate with most students\u0026rsquo; careers beyond MDS once they graduate. Also, one class starting in block 3 until block 6 will supposedly have a partner component where you work together on a project. So, with all that said, can\u0026rsquo;t wait for what block 2 has in store.\n","date":1539227181,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539227181,"objectID":"a1fe83781d5c725ea5dbf3b06d6db6b9","permalink":"https://ehhope.github.io/post/mds-block-1/","publishdate":"2018-10-10T20:06:21-07:00","relpermalink":"/post/mds-block-1/","section":"post","summary":"My experience and thoughts on block 1 of MDS at UBC. The courses were: DSCI 511, DSCI 521, DSCI 542, and DSCI 551.\n","tags":[],"title":"MDS Block 1","type":"post"},{"authors":["Alex Hope"],"categories":[],"content":"This post is on Overfitting, what it is and why it matters.\nOverfitting What is overfitting exactly? Why does it matter?\nThe truth is the word has a few definitions, and depending on who you ask and the context in which the word is used you may receive a wildly different answer. When I talk about overfitting I am not referring to when someone has outgrown their old pants or when a pipe is too large for it\u0026rsquo;s receiver. I\u0026rsquo;m talking about it\u0026rsquo;s use in statistics. Here, overfitting is a common pitfall in data analysis where a particular model explains observations in a dataset too precisely and confidently.\nWait though, wouldn\u0026rsquo;t explaining data accurately and with confidence be a good thing?\nWell yes, accurately detailing trends in a dataset is considered a good thing but if that explanation isn\u0026rsquo;t useful when applying it to future datasets than it defeats the purpose of why we are even here. To predict the future! Just because a model perfectly explains the information in a given dataset does not mean it is a) a great model and b) suitable to explain other observations it has yet to encounter. One sign that your model may be overfitted to your data is when there is very low variance that it fails to account for. In fact, if a model has been trained to predict patterns of data in an original \u0026lsquo;training dataset\u0026rsquo; too strongly, it will struggle with making close approximations about patterns in new datasets that vary in small or large ways that may or may not be interesting (natural variation vs error).\nOf course, we want to build a model that accurately accounts for both present and future observations, however, if the model is overfitted to the present data than it will struggle with predicting new information that do not fit cleanly into the distribution of the original dataset it was trained on. This is a common issue and there are many ways to combat this problem, however, that goes beyond the scope of this topic.\nThe fact of the matter is data is very messy, and will not arrive in identically similar ways because measuring phenomena in the world is a complex and difficult endeavor. A delicate balance always needs to be struck between highlighting interesting variation in a dataset and ignoring aspects that are simply noise, and mark my words, noise will almost always exist.\nThat was a lot of words, let\u0026rsquo;s break it down visually. What are examples of different kinds of fittings? Below you will find examples of a) underfitting, b) appropriate fitting, and c) overfitting that outline how different quadratic functions of models can be fitted to data. Bear in mind, not all models are made equal!\nHere are examples of different \u0026ldquo;fits\u0026rdquo; on the same data:\nThe images above represent three examples of different functions that capture trends in a set of observations (i.e \u0026lsquo;X\u0026rsquo; and \u0026lsquo;O\u0026rsquo;). In each of the above pictures the aim of the model is to best capture the position of \u0026lsquo;O\u0026rsquo; and \u0026lsquo;X\u0026rsquo; datapoints as they are distributed across the x- and y-axis. However, some capture the data better than others. The far right image is an example of overfitting where the function has been forced to fit all of the \u0026lsquo;O\u0026rsquo; observations and all of the \u0026lsquo;X\u0026rsquo; observations. As you can see it captures it perfectly! But how do you think it would hold up if a new dataset was presented on the same 2-plane axis? Probably not very well since observations tend to move at least slightly within a distribution and if any of these data points were to move in the axes, the model would hold much less predictive power. This is what is meant by picking a suitable model over a perfect model. We are interested in generalizing our predictions to other phenomena, not perfectly modeling observations that already exist, anyone can do that.\nConversely, on the left image you have under-fitting where the model hasn\u0026rsquo;t been trained adequately to grasp patterns in a dataset that are clearly meaningful signals. In those cases the predictive power of the model will be impaired as well, and so the delicate balance between both extremes leads to an appropriate fit shown in the middle. Here, you can see that the appropriate fit does not capture the data perfectly, however, it does do an excellent job at explaining the general direction of the trend while leaving enough room for observations to potentially fall on either side of the quadratic function. Essentially, this model predicts the trend well but also leaves room for error within the distribution, which will almost always exist.\nWhy does this topic even matter?\nThis topic matters a ton! In the current data-driven world we inhabit, humans are increasingly relying on data to aid their decision-making across both their personal and professional lives. Making an informed decision about anything hinges on the quality of the information being received, and the estimations made based on that information. We do this by building effective, properly fitted models that make accurate predictions about the future. From marketing to production lines to brain-imaging to product design and consumption, analytics play a role across many facets of society. My hope is that you now understand what the term overfitting refers to in statistics, the consequences of overfitting statistical models and what the balance is when fitting a model to data.\n","date":1537076486,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537076486,"objectID":"23dad439a26b14f42602392bd4839f21","permalink":"https://ehhope.github.io/post/overfitting/","publishdate":"2018-09-15T22:41:26-07:00","relpermalink":"/post/overfitting/","section":"post","summary":"This post is on Overfitting, what it is and why it matters.\n","tags":[],"title":"Overfitting","type":"post"},{"authors":["Alvarez, L., Wiebe, S.A., Adams, K., Hope, A., \u0026 Cook, A."],"categories":null,"content":"The strong relationship between motor and cognitive development suggests that the limited motor experience of children with physical disabilities can impact their cognitive and perceptual development. The assessment of their cognitive skills is also compromised due to limited verbal communication and motor gestures. Robots have been used to give children with disabilities an opportunity to independently manipulate objects and to reveal their cognitive skills when they use the robots. Little is known about the neural correlates that subtend robotic augmentative manipulation and the ways in which using a robot to manipulate objects may change the task\u0026rsquo;s cognitive and perceptual demands. Several technical considerations pose a challenge to such studies.\nOBJECTIVE: This paper presents a methodology for the technical implementation of neurophysiological exploration of robot-augmented manipulation and presents an evaluation of the technical feasibility of performing a comparison between augmented manipulation and direct manipulation as response modalities in a cognitive task.\nMETHODS: A costume made interface was designed that would allow the interfacing of the EGI NetStation Electroencephalographic (EEG) signal acquisition system, the E-Prime stimulus presentation system, and a 3-Dimensional task performed with either a robot or through typical direct manipulation. The technical feasibility and the stability of the designed technical implementation was tested with 10 adult participants.\nRESULTS: Initial analysis revealed specific robot control interface related artifacts. Further testing confirmed the source of artifact. Independent Component Analysis (ICA) was successfully used to separate this artifact component. Advantages, disadvantages, and results obtained from this method for technical implementation are presented. Implications for the study of neural correlates of augmentative manipulation are discussed.\n","date":1396403478,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396403478,"objectID":"4a1eb81794032ac0880bf683e8190754","permalink":"https://ehhope.github.io/publication/neurophysiology/","publishdate":"2014-04-01T18:51:18-07:00","relpermalink":"/publication/neurophysiology/","section":"publication","summary":"The strong relationship between motor and cognitive development suggests that the limited motor experience of children with physical disabilities can impact their cognitive and perceptual development. The assessment of their cognitive skills is also compromised due to limited verbal communication and motor gestures. Robots have been used to give children with disabilities an opportunity to independently manipulate objects and to reveal their cognitive skills when they use the robots. Little is known about the neural correlates that subtend robotic augmentative manipulation and the ways in which using a robot to manipulate objects may change the task\u0026rsquo;s cognitive and perceptual demands.","tags":[],"title":"The Neurophysiology of Motor Manipulation","type":"publication"}]