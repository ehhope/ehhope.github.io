[{"authors":[],"categories":[],"content":"My experience and thoughts on block 5 of MDS at UBC. The courses were: Collab Soft Dev, Regression II, Time/Spat Analysis, \u0026amp; Bayes.\nMDS Block 5 Alright, here I am after another block of coursework in MDS. If you missed my discussion of block 4 you can find it here or any other blocks on my home page under blog. This blog post will be a bit shorter, but nonetheless convey my thoughts on each class. Ok,let\u0026rsquo;s dive in.\nUnlike block 4 where we focused entirely on the intricacies of machine learning, feature selection, and deep learning, this month covered the classical statistical approaches in R. For myself, this was bar-none the most challenging set of coursework to date, and can speak on behalf of many others in the program who feel similar. Coming in, I had next to no experience with generalized linear models, implementing bayesian models using RJAGS, or how to assess and extract time series data for seasonal or linear trends and forecast this time-series data. These are all vital skills in a Data Scientists toolkit that are heavily relied upon, so it was worth the effort I invested in theses classes to walk away with the understanding I have. Needless to say, a lot has been learned, and I\u0026rsquo;m excited to grow even further in these areas and apply my understanding to data in the wild. The fourth course was Collaborative Software Development (DSCI-524) and was the group project for the block. Given the freedom to develop any package we desired, our team built LibRely a library available in Python and R that extracts functions and dependencies from other scripts you provide as input. Developing the package was enjoyable, but the real learning for me came from developing software tests, using testthat and pytest as well as working with continuous integration setups such as Travis.\nRegression II (DSCI-562 - Instructor: Vincenzo Coia) Building on a previous course on the fundamentals of regression (DSCI-561), this class extended our understanding of simple and multiple linear regression to other forms of data that can be interpreted with linear models. You\u0026rsquo;ll learn about how to deal with response variables that are count data (ex. poisson regression), the consequences of transforming your data versus your model function (ex. log(y) vs link = \u0026ldquo;log\u0026rdquo;)), survival analysis, ordinal regression, and different forms of data imputation (ex. mean, multiple, etc.). Vincenzo taught me about the power of linear mixed models (LME), how common these approaches are used across domains because of their utility in identifying general trends in groups and effects within subgroups. Of course, as we increase the number of parameters in our model our degrees of freedom will lower, and this is a consequnce of LME\u0026rsquo;s, however with enough data it becomes less of an issue.\nVincenzo was in my opinion the best instructor this block, in part because he is a core MDS faculty member, and tailors his content with a detailed understanding of the cohort while connecting the material with our labwork. There was very little disconnect between his lectures and labs, and he can explain statistical technqiues clearly and in numerous ways.\nSpatial \u0026amp; Time-Series Analysis (DSCI 574 - Instructor: Natalia Nolde) This was one of the harder courses in the block, however, I found the instructor made the material even more difficult than it already was. Natalia is clearly a strong statistician, with an intimate knowledge of time-series and spatial processes in data, but she was more concerned about covering the concepts than ensuring students possessed understanding in her lectures. I learned time-series analysis by reading a TON of texts, and referencing both her notes and datacamp where it was convenient to support my learning. So what did I learn? For one thiing, how common it is to encounter time-series data in the wild, and as a result how important it is to be familiar with the models we use to extract patterns out of this kind of data. For example, you\u0026rsquo;ll learn about ARMA and ARIMA models, which are still on the basic side of time-series analysis and compromise both moving average and autoregression models in their process. These models look for patterns across different lengths of time (lag), and the nature of the patterns such whether they are cyclical or trending in a certain direction, or perhaps both. We can use these trends to make projections outward beyond our known series to forecast the future value of a given variable at time t.\nYou\u0026rsquo;ll also learn about relationships in spatial data, and these generally adhere to similar concepts as time-series data except using a coordinates system instead of a time variable. Processes such as spatial dependence, variograms, and kriging are all important tools when examining the patterns of values in space. Unfortunately, I had to learn about these on my own as well since our instructor wasn\u0026rsquo;t able to complete time-series until later than she had anticipated, leaving little time to digest the spatial concepts. Overall, this course is a fascinating topic, I just wish it would have been covered more effectively, and by someoene who understood the constraints of MDS.\nBayesian Inference (DSCI 553 - Instructor: Alexandre Bouchard-Cote) You would be hard-pressed to find a student in MDS that found this class easy. Thankfully, I felt it had very little to do with the instructor, it was just a class with very difficult concepts crammed into a month. You\u0026rsquo;ll learn that bayesian statistics is truly the counter point to frequentist approaches and what you\u0026rsquo;ve been taught most of your life. At the heart of bayes is the assertion that we must model uncertainty, not simply trust the data we have to speak about the likelihood of an event occuring. In reality, we almost always have opinions about the probability of an event occuring (prior), and as we encounter reality and events relating to these beliefs (likelihood), we update our beliefs (posterior) to fit the evidence reality gives us about an event. This is essentially what we do in bayesian analysis, only we simulate these experiences many many times using a probabilistic programming language (PPL) known as RJAGS in order to hopefully converge on an estimate about the chance of an event occuring for example.\nThe challenge with this class is really learning what the vital concepts (distributions, prior, likelihood, posterior, MCMC, etc.) mean and how they interact in a bayesian model when you merge your conceptual understanding with RJAGS to run your analysis. Once you can reorient yourself around this idea of uncertainty, you\u0026rsquo;ll find the maneuver from the prior to the posterior to be an easier task. While many had complaints about the instructor, I found him to be helpful for building my intuition around these core concepts, and that he took unwarranted criticism stemming from engaging bayesian thinking. All in all, a difficult but rewarding course.\nCollaborative Software Development (DSCI-524 - Instructor: Meghan Allen \u0026amp; Jenny Bryant) Overall, our team worked extremely well together in developing our package from start to finish and I\u0026rsquo;m very proud of our work. My only real issue with this class was that we are here to be data scientists, not software developers, and study time is crucial in this degree. While some of us may move onto developer roles within organizations, most wanted to prioritize learning statistics. I had already known about pytest, version control, and debugging prior to the course, and knew these weren\u0026rsquo;t essential to being a data scientist. They are undoubtedly useful, but I wanted to strictly delve into statistics this block, and found the course to be a nuissance despite building a great package with my teammates.\nYou\u0026rsquo;re in great hands with Meghan \u0026amp; Jenny, they have clearly done this before and know how to guide you in the right direction as your group develops the package and moves through various stages of the development process.\nWhat a block this was! I can\u0026rsquo;t believe how much I was able to learn, and apply in such a short-time, and I intend to build on this knowledge in the coming months as I consider work opportunities and the announcement of our capstone project! Hopefully you found this summary a helpful guide for what to expect from these classes, and I\u0026rsquo;ve painted a sense of the dedication required to succeed in these courses. Until next time!\n","date":1552872404,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552872404,"objectID":"0275a18e36b4e4d7e99e994d9aed5b11","permalink":"https://ehhope.github.io/post/mds-block-5/","publishdate":"2019-03-17T18:26:44-07:00","relpermalink":"/post/mds-block-5/","section":"post","summary":"My experience and thoughts on block 5 of MDS at UBC. The courses were: Collab Soft Dev, Regression II, Time/Spat Analysis, \u0026amp; Bayes.\n","tags":["Bayes","Time-Series","Spatial Analysis","Statistics","Software Development","Travis","MDS","Data Science"],"title":"MDS Block 5","type":"post"},{"authors":[],"categories":[],"content":"My experience and thoughts on block 6 of MDS at UBC. The courses were: Cloud Computing, Privacy \u0026amp; Ethics, Causal Modeling, \u0026amp; Advanced Machine Learning Methods.\nMDS Block 6 Alright, I am here to write about the final block of MDS, what can I say it\u0026rsquo;s been a long journey through the degree filled with many ups and downs. If you missed my discussion of block 5 you can find it here or any other blocks on my home page under blog. With that out of the way let\u0026rsquo;s dive in.\nI can confidently say that Block 6 of MDS was much easier than the prior two blocks of coursework. This was clearly a conscious decision on the part of MDS faculty who recognized some students were quite burnt out. That said, it was far from a cakewalk, particularly Advanced Machine Learning and Causal Modeling.\nI\u0026rsquo;m going to offer a small rant about each course, but not spend a ton of time on each per se. I\u0026rsquo;m more interested in offering a high-level overview than getting into granular details about a specific course. So, let\u0026rsquo;s start.\nWe begin with Cloud Computing (DSCI-525), which taught me a ton about the power of distributed computing in today\u0026rsquo;s big data age, how to scrape web information from HTML pages and leverage API\u0026rsquo;s in my own projects.\nI had always heard about map/reduce frameworks but this was the first time I was exposed to it using Amazon Web Services (AWS). The power of AWS is quite astounding, even with the free version they offer or the extremely cheap clusters you can access. You\u0026rsquo;ll learn about how to handle massive amounts of data and offload the computing power to several computing sources to do your computation, which in many cases will be complex and fry your computer if you do it yourself. Mike Feeley has a wicked sense of humour, and is a great lecturer. I know he was controversial to the cohort but I really thought he delivered material that was interesting and pragmatic at the same time. Solid instructor.\nNext is Experimental \u0026amp; Causal Inference (DSCI 554), which was a fantastic walkthrough of the considerations required for performing strong statistical analyses, particularly AB tests. Depending on your role in a company AB testing may or may not be a central responsibility, but the journey of walking through how to sample and test for group differences in A and B is more complex and a lot of foresight is required to ensure you\u0026rsquo;re comparing groups effectively and fairly. Paul Gustafson taught this class and he really was a master at conveying the concerns in a step wise fashion for AB testing, and reiterated over and over the considerations required for distinct parts of the process from sampling and controlling for group charactersitics to performing many tests (Bonfferoni tests, etc.) and the tentative conclusions that can be drawn from AB tests. I won\u0026rsquo;t go much further into this class but to summarise, it took all of the stats we learned over the last year and showed us how to be aware of how we are applying these techniques. Being good at stats is part knowledge of techniques, but also knowledge of the assumptions you are making in applying those techniques. He did a great job of raising our awareness on the latter.\nThird was Privacy and Ethics (DSCI 541), which in my opinion was a big of waste of our time. It\u0026rsquo;s not say Data Scientists shouldn\u0026rsquo;t be aware of the concerns that arise from data around individual exposure and hte power of tech companies that essentially monitor their users and sell this knowledge to companies. It\u0026rsquo;s the fact that a lot of what we were taught seemed to be core expressions of ethics that were obvious, and didn\u0026rsquo;t require a class. It devolved more to social and political stances than actual learning about how to address deep privacy concerns, and the discussion would often fall into a moral relativistic tone which felt silly and a waste of my time. The instructor really cared about the class and students opinions on various topics, but I can\u0026rsquo;t say what I learned is indispensible or all that useful in my own work. Companies have their systems in place, I am not there to question their practices in privacy and if I do see something I really don\u0026rsquo;t think the class helped me spot that concern any more than common sense. Ed Knorr made the quizzes really tough because he cares deeply about the course and wants to challenge students, but it was hard to take the course seriously at times because of the points I mentioned above. I did learn about hashing though and how public and private keys actually function on the web, so I\u0026rsquo;ve got that in my pocket now\u0026hellip;. In general though, would\u0026rsquo;ve liked a practical stats class where we apply everything we learned on different problems for a month instead.\nFinally, the last class is Advanced Machine Learning Methods (DSCI 575) with Varada Kolhatkar. This was her first time teaching, and in general did an awesome job introducing us to high-level topics in ML. We started on Markov and Hidden Markov Models, as well as prepping data for NLP using co-occurrence matrices and n-gram formats. We then shifted to Recurrent Neural Networks and LSTM\u0026rsquo;s, which are used across natural language and quantitative problems and I found the complexity of LSTM\u0026rsquo;s to be quite fascinating. It took me back to my days in neuroscience where we aren\u0026rsquo;y strictly examine neuronal activation, but the processes within the cell that propogate that signal. LSTM\u0026rsquo;s contain distinct gates that serve different purposes within the single network, and these signals are then passed onto the next network and remembered within the same one. It was a great class in general, the quizzes were quite easy but the labs allowed me to dig deep into Markov and NN models. This block I would say had the best group of instructors of any block, all four were pretty solid people who cared about the students and their digestion of the material.\nI can\u0026rsquo;t believe I just finished 6 blocks of coursework, and 24 classes of material. It\u0026rsquo;s been a long journey to this point and the amount of information I\u0026rsquo;ve learned is hard to even fathom. I can see the challenge will be to take what I either consciously know or residually still have from early blocks and apply it in my work to formulate the foundation that serves the direction I want to move with a career. I really believe if you do not practice what you learn from this degree you won\u0026rsquo;t retain what you were taught, plain and simple, there is too much to know. That said, you can go any direction you want from here\u0026hellip;. viz, ml, stats, software dev even if you want. The personal freedom this knowledge grants is pretty astounding.\nLooking back though this degree has given me the tools to move out into the world and apply my knowledge to a variety of problems that all seem interesting. Personally, I love the diversity of applications in ML and the fact it enables other industries to simply perform at a higher level. The diverse applications is what excites me, and so I\u0026rsquo;ve been fortunate to take a position as an ML intern for 5 months at a tech consultancy, where I\u0026rsquo;ll be tackling healthcare and business oriented problems and I can\u0026rsquo;t wait to get started.\nI may make a summary post of my thoughts on the degree, and whether I got the value out of the degree that I had thought I would. But for now I\u0026rsquo;m off to work on my capstone for the next 2 months. I hope some of you found these posts worthwhile if you were applying to MDS, a fellow student in the program or just a curious inquirer. I will definitely be posting more in the future though so stay tuned for that if you enjoyed!\n","date":1552872404,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552872404,"objectID":"5590e91417e36f356575a53ae09d0c0b","permalink":"https://ehhope.github.io/post/mds-block-6/","publishdate":"2019-03-17T18:26:44-07:00","relpermalink":"/post/mds-block-6/","section":"post","summary":"My experience and thoughts on block 6 of MDS at UBC. The courses were: Cloud Computing, Privacy \u0026amp; Ethics, Causal Modeling, \u0026amp; Advanced Machine Learning Methods.\n","tags":["AWS","Cloud Computing","Ethics in Data Science","Statistics","NLP"],"title":"MDS Block 6","type":"post"},{"authors":["Alex Hope"],"categories":[],"content":"My experience and thoughts on block 4 of MDS at UBC. The courses were: DSCI 532, DSCI 563, DSCI 572, and DSCI 573.\nMDS Block 4 Another month another block, if you haven\u0026rsquo;t read my past reviews you can find the previous summary of Block 3 here or any of the others at the home page of this site. Generally speaking, the coursework in this block was one of the major reasons I decided to apply to MDS in the first place\u0026hellip;. machine learning, machine learning, machine learning. Three of our four courses were centered on ML with a fourth class that was part II of visualization. A ton and I mean a ton of material was covered in this block plain and simple, and I\u0026rsquo;m not here to regurgitate everything I\u0026rsquo;ve learned. Let me offer some meta-thoughts on the structure of the program this block, and some opinions on the ML and Viz coursework.\nOverview This is considered the toughest block of coursework in MDS for a reason. The difficulty of the content and large amount of material you are asked to learn in each course for the month it is covered is asking alot of students. I spent many, many nights late at the library or in my room reading resources, watching videos, and deciphering code in addition to my lab work. The workload definitely translated to burnout I could see on the faces of many students, including myself at times. I wouldn\u0026rsquo;t say these expressions are a bad thing though because it was also a very valuable month. On the one hand I learned a lot about ML, but more then that I realized how deep this field is, in what I learned content-wise and for my own perspective. Standing at the top of the block 4 mountain, I\u0026rsquo;m realizing two camps seem to have emerged that roughly summarize student sentiment about this block.\nCamp 1: These students are burnt out and frustrated with the difficulty of the material and labwork, and don\u0026rsquo;t believe the program is structured well to deliver what they feel MDS should\u0026hellip;.. a thorough understanding of machine learning. This camp has high self-expectations and wants to understand on a very granular level how everything works, which is admirable for such a tough topic but not very realistic if your background isn\u0026rsquo;t comp sci or math and your studying ML for a month.\nCamp 2: These students are also burnt out and have a hard time with the material and labwork, however, they see that some of the math behind ML algorithms and the code that represents them is not deeply acquired in the time they are given plain and simple. Because of this they believe that MDS\u0026rsquo; role is to introduce us to these complex topics not help us master them. They know they arne\u0026rsquo;t walkiing away with a PhD, but if they work hard they can learn a ton and lay a great fundation for future work. The darker areas around math and some complicated code simply need to be explored more in capstone or once you graduate.\nThis isn\u0026rsquo;t to say there aren\u0026rsquo;t other opinions because there are, but these were two very common themes I kept hearing and thinking about myself week after week. Both camps make reasonable points about what MDS should offer, and I can say for myself I started in Camp 1 and as each week progressed slowly moved over to Camp 2.\nData Science is a really hard field to grasp because it requires a breadth of tools to follow an idea through, but also a depth with those tools to generate the real value. It\u0026rsquo;s not a fault of the program for being unable to condense something very complicated it\u0026rsquo;s just the state of the field, in fact, they did an amazing job across the board. The entire MDS staff was very sensitive to the fact it was a difficult month for students, and made as many accomodations for us as they could. Without their support, this could\u0026rsquo;ve been a much tougher experience so big thank you to them. Ok, without further adue, the coursework.\nMachine Learning Coursework  Unsupervised Learning (DSCI-563, Rodolfo Lourenzutti)\n Supervised Learning II (DSCI-572, Mike Gelbart)\n Features and Model Selection (DSCI-573, Mark Schmidt)\n  These courses all covered machine learning in some capacity, with significant overlap so I\u0026rsquo;m going to clump them together here. Here are some examples of the questions you address in this block: How do we group/classify unlabelled data? How do algorithms optimize their loss when they are \u0026lsquo;fitted\u0026rsquo; to data? How do we figure out if our features are helping or hurting model performance? If we have too many dimensions (features) in our data how can we reduce them and visualize relationships? How do we build neural networks and what advantage do convoluted neural networks have over fully connected networks? How does Amazon\u0026rsquo;s recommendation system work?\nThe main focus can be boiled down to two basic questions: 1) How do neural networks work? 2) How do we \u0026lsquo;tune\u0026rsquo; algorithms so that we optimize their performance and what things do we need to consider in order to do this? Well, you need to pick your base model, choose your features wisely (domain-specific expertise), optimize your hyperparameters, consider your loss function and how you penalize errors in your model. All of these considerations are delicate questions that aren\u0026rsquo;t answered easily and have massive implications for how your model will perform. For example, picking how your model penalizes error (L1 or L2 regularization) has significant effects on model performance becuase you are handling \u0026lsquo;error\u0026rsquo; in completely different ways. We saw this in the context of linear regression where our penalizer would dictate the number of features we actually used in our model to predict targets with L1 keeping a significant number of features and L2 attempting to reduce as many feature weights to 0 as possible.\nSo how do full and semi connected neural networks work? Well, you\u0026rsquo;re going to find out when you start building them with a library called \u0026lsquo;Keras\u0026rsquo;. You\u0026rsquo;ll learn a bit about other packages but Keras is a very accessible way to start building quickly because it\u0026rsquo;s intuitive and simpler than other libraries. It\u0026rsquo;s pretty amazing that anyone can build a massive network in 4 lines of code now\u0026hellip;. just specify your neurons in your input, hidden layers and shape of your output before you compile and you\u0026rsquo;ve built a neural network. Now, it\u0026rsquo;s simple on the surface when I explain it that way but the more you start building the more you realize the difficult of building them well. Nonetheless, the ease in which anybody can build a NN is pretty profound. Unlike the majority of algorithms we\u0026rsquo;ve learned to this point, you have much more control over the architecture of a neural network than anything you\u0026rsquo;ll learn in sklearn. You can have a hidden layer with 5 neurons or 100 if you want, the only real restrictions you may have are the inputs and outputs depending on the data you are working with. Mike Gelbart knows his material extremely well, and is talented at explaining complicated topics from many angles so that students can understand what\u0026rsquo;s happening even if they miss one perspective of what he\u0026rsquo;s explaining. It\u0026rsquo;s very helpful in that sense, and I\u0026rsquo;m glad he taught this course given his history with us in this program.\nFor DSCI 563, we covered a range of content tied to clustering techniques (K-Means, K-Medians, DBSCAN, Hierarchical Clustering (linkages) and dimensionality reduction approaches (Principal Component Analysis (PCA) and Non-Negative Matrix Factorization (NMF)). The essential question of this class is, how can I make sense of data that isn\u0026rsquo;t labelled and utilize different algorithms to characterize relationships in this data? Using the techniques I described above, you can tackle a range of different questions and arrive at some very interesting solutions. You will learn how different mathematical approaches (euclidean, cosine similarity, etc.) can model \u0026lsquo;similarity\u0026rsquo; between items in Amazon\u0026rsquo;s database. You\u0026rsquo;ll also learn how images of faces can be condensed and reconstructed using PCA and NMF, and understand the differences in how these techniques function in both their reduction and reconstruction. The face data is well chosen because it helps build your intuition about how these methods work when you can compare original faces with reconstructed ones and gauge their similarities/differences between the two. Let\u0026rsquo;s just say if you take enough components you can store high dimensional data extremely accurately, and reconstruct that information with high precision. These are very powerful techniques that any data scientist needs to have in their toolbox, and Rodolfo did a solid job of explaining concepts in intuitive ways while only bringing in heavy math when it was needed.\nVisualization II (DSCI-532, Cydney Nielsen) This class admittedly did not get enough attention from students as it deserved. The reason for this is because students were completely absorbed by 3 classes on machine learning that it inevitably became the \u0026lsquo;tag along\u0026rsquo; course that students focused less on. That said, I thought Cydney was a great instructor who knew a ton about data visualization and how to display information in impactful ways for different audiences. You could tell she had ample experience in visualization as a computational biologist, and that it translated to her work as a data scientist with Microsoft quite well. Her lectures were a basic run down of \u0026lsquo;the science of visualization\u0026rsquo;, a crash course in human perception and graphing theory. This course contained the group project for block 4, where we were assigned a partner and taught to build Shiny dashboards for displaying data. I can confidently say that I can go to any company now and build simple, sleek dashboards that help people visualize information they care about. Not only did I learn the code behind shiny dashboards, but just as importantly how to think about design processes for the end consumer. Your target consumer will dictate the kind of information you convey from a dataset, how you present the information and the aspects you allow the user to customize to address their questions. These are all valuable skills a designer needs, and the class will definitely help you from a coding and design perspective.\nI could talk for days about this block, but I think what I\u0026rsquo;ve painted a detailed picture to convey my feelings on the program for the last month and the topics we covered. Hopefully you found this information helpful, and see you at the end of block 5!\n","date":1549155725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549155725,"objectID":"5bf1b6dee78c51797ec20a775aea89e0","permalink":"https://ehhope.github.io/post/mds-block-4/","publishdate":"2019-02-02T17:02:05-08:00","relpermalink":"/post/mds-block-4/","section":"post","summary":"My experience and thoughts on block 4 of MDS at UBC. The courses were: DSCI 532, DSCI 563, DSCI 572, and DSCI 573.\n","tags":["Machine Learning","Regression","Shiny","Neural Networks","Optimization","MDS","UBC"],"title":"MDS Block 4","type":"post"},{"authors":["Alex Hope"],"categories":[],"content":"My experience and thoughts on block 3 of MDS at UBC. The courses were: DSCI 513, DSCI 522, DSCI 561, and DSCI 571.\nIntroduction to Block 3 This post comes to you after finishing block 3 of MDS, the final block of 2018 and the 3rd installment in this blog series. If you haven\u0026rsquo;t already, check out my post about the 2nd block of this degree so far.\nWhat a month it was! I think I speak for everyone in the program and say this was the most intellectually challenging yet gratifying work in the degree thus far. Quite simply, you can start to see the power behind what you are learning in classes like Supervised learning (DSCI 571) and Workflows (DSCI 522) and the importance of having instructors curate content that understand it deeply and know what you need to focus on and what you can learn on your own or ignore. Any student that went through this block can tell you about the basics of linear regression, how relational and semi-structured databases work, how to apply different machine learning algorithms for different problems and the pros/cons of each, as well as how to automate scripts that span the analysis process (wrangling, visualization, analysis and communication). It felt like we were starting to dig into the material that brought me here in the first place, and the content that was the real value I could offer an employer on the other side of this master\u0026rsquo;s program. Without further delay, each class from block 3.\nSupervised Learning (DSCI-571: Dr. Mike Gelbart) Heading into this class I was a bit intimidated looking at the material. Anyone with an interest in data science has heard of sci-kit learn, machine learning, overfitting and underfitting, but I also knew these could be incredibly complex concepts that were not easy to unpack\u0026hellip;. let alone in 4 weeks. To my surprise, I learned more in this class than any other I\u0026rsquo;ve taken in this degree, hands down. Mike Gelbart delivered a \u0026ldquo;flipped\u0026rdquo; classroom style course where before each lecture we were asked to watch a video on a specific machine learning classifier that we would then cover in lecture. The opportunity to absorb information prior to lecture was a huge help in scaling my learning, and allowed me to go deeper into the concepts we covered in lecture as a result. Mike\u0026rsquo;s examples and notes were extremely clear and helpful, and when paired with the labs I could see the power of the sci-kit library and how many of the classifiers could be applied to a range of datasets from music rankings (Spotify), voting patterns in political elections (Democrat vs Republican), to natural language processing (IMDB Movie Reviews), just to name a few. We also covered critical concepts like cross-validation, balancing your training and test error and how determining the \u0026lsquo;accuracy\u0026rsquo; of a given model isn\u0026rsquo;t always as straight-forward as it may seem. Any student leaving this class is capable of: applying machine learning algorithms with the fundamental trade-off of selecting hyperparameters kept in mind, and understanding what algorithms may be best for classification or regression problems.\nThere is so much to learn in this area and while it feels like we\u0026rsquo;ve still scratched the surface on how different algorithms actually work underneath the hood (math and assumptions represented as code within each function) and perform under certain conditions with different kinds of data, it taught me that these topics don\u0026rsquo;t have to be intimidating to get into. In fact, if you focus on a high-level understanding of what\u0026rsquo;s happening when an algorithm is fit to data and predicts on new data and consider WHY some algorithms may be more effective than others at this process, you can walk away with a great foundation of machine learning fundamentals. For anybody whose new to concepts in data science, there can be a sense of fear in students when confronting machine learning courses and understandably so because they are indeed complex topics. While there are a plethora of materials available out there to learn on your own and it has never been easier, what really made MDS worth it is to know what to learn within ML, in what order to best digest the info, and to have someone take you through simple examples of how it can be applied and present important concepts to you in a way that are gradual to help build your intuition and grasp of the information.\nWith that in mind, I have to thank Mike and Varada (Head TA) for designing this curriculum in a way that was extremely digestible. I had heard rumours that DSCI 571 was quite difficult for the previous cohort and I think they both took this to heart in designing the curriculum and labs this year, hats off to them. I also consistently got the impression they cared about the success of students in the course, and would do whatever is needed to help you understand concepts whether its extending or adding office hours, recommending new materials or addressing slack questions promptly.\nFor any future students, coursework like this is likely a core reason why you are applied to MDS, and I can happily say it is also where you will feel like you\u0026rsquo;re growing the most in this degree and I\u0026rsquo;m glad I feel I made the most of this opportunity.\nData Science Workflows (DSCI-522: Dr. Tiffany Timbers) This class was interesting because it was the first of it\u0026rsquo;s kind up until now for this year\u0026rsquo;s MDS cohort. While I had heard issues raised from some students in my cohort about fixed group work for the entirety of the course, I found it to be a great experience with my group partner. This field is an extremely collaborative one, and I know I valued the experience of working through both technical and social roadblocks with a partner. The group work was one unique aspect but it also did not have any exams or labs, only project deadlines each week that asked us to apply what we had learned in lecture regarding script automation and containerization to our project. My partner and I chose an NBA dataset where we applied our supervised learning skills (decision tree classifier) to try and predict whether a shot would be made or missed by Lebron James, and automate our wrangling, exploratory visualization and analysis process. It was an interesting blend of many of the skills we acquired in block 2 regarding visualizations, wrangling as well as the block 3 machine learning skills we cultivated.\nThe real focus of the course wasn\u0026rsquo;t reiterating these skills that we had already acquired though, it was using tools like Make and Docker to automate these processes and encapsulate them in containers that are reproducible across platforms. I intend on applying them to all of my projects in the future since once the framework is built they can deliver faster, more flexible results. For instance, my partner had an ingenius idea to take our project a step further in automating our scripts based on a keyword argument so that when we ran the scripts and provided a \u0026lsquo;player name\u0026rsquo; argument, you could run the full report for any player in the NBA you wanted, not just Lebron James. Now you can wrangle, visualize, and analyze characteristics of shots from any player in the NBA and compute these scripts in under a few seconds. Amazing!\nMy hope is that in the future I can use Make and Docker to build more complex and flexible scripts than the project we built, perhaps a larger analysis pipeline to address questions that interest me and allow me to come back and run new ideas through a pipeline that I think may be interesting. Once the pipeline is built, as long as you are feeding it data that is structured to fit your code, you can generate results in very little time and that seems like a valuable investment for future work with similar datasets and approaches. I highly recommend becoming acquainted with script automation and firmly stand by the belief that it is worth it to invest yourself in this class. If you\u0026rsquo;re working within a larger company when you graduate, there is a very good chance that they will have large pipelines in place already, and you will be asked to take advantage of these in your work. It will save you a ton of time and energy, and you\u0026rsquo;ll be more productive in the long run. So dive in!\nTiffany\u0026rsquo;s teaching style really shines through in courses where there\u0026rsquo;s a lot of in-class coding and actionable instructions to follow in lecture. Her strength as an instructor is taking various packages and platforms that are complex and showing students in a step-wise fashion how the basics actually work. She is great at helping you realize this doesn\u0026rsquo;t have to be so complicated. Every lecture Tiffany also gives us a 5-minute break and plays music, which was an unexpected but greatly appreciated mental break. I know she is mindful of our workload and stress levels, even with the little things like this. Some students had issues with their partner or some aspect of the group work but for myself, it was a great experience where I could learn lots from both Tiffany and my partner but also not sweat looming deadlines of labs or exams for the class. This helped with balancing labs and exams for the other classes as well.\nFor anyone interested in the NBA project my partner and I worked on for this class, it can be found on GitHub here: Lebron James for T(h)ree!\nRegression I (DSCI-561: Dr. Gabriela Cohen-Frue) Heading into this class, I knew exactly what to expect and that it wouldn\u0026rsquo;t be easy for me. I had encountered linear regression in one of my past positions in mental health research, but admittedly, did not fully understand the ins-and-outs of what I was assuming and had generally struggled through statistics classes in my undergrad. Undoubtedly, I worked the hardest in this class to understand the material, but I can only imagine how much harder it would have been if Gabriela had not been our instructor. I thought this class moved at a reasonably slow pace, where we really focused on the core components of simple and multiple linear regression across 7 of the 8 weeks. Gabriela re-iterated many themes in this class at both the theoretical level with respect to the thinking behind concepts such as cell-means versus reference-treatment parameterization, errors vs residuals and least squares fitting, but also how this was expressed in R\u0026rsquo;s output.\nUltimately as data scientists we will be working with a programming language that computes many of these statistical measures, and so the burden of understanding falls more on the side of using R\u0026rsquo;s functions and interpreting their output than explicitly calculating the measures and remembering formulas. I thought in general she balanced these demands quite well, but think she could stray a bit further from the calculus aspects of how certain measures are calculated and get back to R. I really wanted a deeper discussion of the direct R output and the higher-level forms of the concepts we were discussing like multicollinearity, and the coefficient of determination, among others. With only 4 weeks to learn regression spending a lecture or two on the calculus behind formulas simply isn\u0026rsquo;t productive in my opinion. However, in general she is a wonderful instructor with an intimate understanding of statistics that can\u0026rsquo;t be understated. Ask a ton of questions in this class, I promise you\u0026rsquo;re not alone in wondering what that standard error formula means, how to manage outliers, or different forms of fitting regression models. This is no fault of the instructor, statistics is a slippery fish of knowledge that takes a lot of effort and attention to not only understand but apply properly. The more questions you ask and the more you can remain patient in your learning, the firmer the grasp you will have on this statistical fish.\nI thought Vincenzo (TA) did a solid job with the lab design as well, gradually building our intuition of how regression truly worked while challenging us to consider what the output of lm vs anova vs aov really meant. Overall, this course was a pleasant surprise given my past struggles in statistics. Gabriela and Vincenzo will help you see the value in understanding these concepts as they will undoubtedly arise in an analytics career. Examining relationships between data, fitting linear models and making predictions about future observations are all critical skills in an analysts toolbox, choose to cultivate them!\nDatabases and Data Retrieval (DSCI-513: Bhav Dhillon) Heading into this class I had no experience with SQL or really any relational or semi-structured databases and while I did finish with a reasonable understanding of the materials, this class left a fair bit of room for improvement. A common trend I am noticing within the degree is that the classes with instructors who are furthest removed from the computer science/MDS/statistics faculty tend to be the worst from an educational standpoint. I don\u0026rsquo;t necessarily say this with respect to their ability as an instructor because I don\u0026rsquo;t think that\u0026rsquo;s the case at all, in fact they have been very talented and driven instructors. What I mean is it\u0026rsquo;s harder to integrate material from the lecture into the labs and draw from past successes in other classes that may be useful for the instructor if they communicate less with the core faculty that curates the overall curriculum. This gap in communication reflected itself in general student engagement, and concept acquisition in DSCI 513. The instructor was a post-doc brought in to teach DSCI-513 and who was not embedded within the overarching educational picture of the program and was not familiar with the content we had already learned that may inform their teaching (ex. inner joins, select, etc.). Bhav was brought in to teach this specific class and the lectures were quite poorly laid out with respect to how they integrated with labwork, plain and simple. That said, she was kind, knowledgeable and always available for help and was much more effective one on one with students I found. Again, I don\u0026rsquo;t think it\u0026rsquo;s a reflection of her abilities at all, simply an organizational critique. The overarching concepts were laid out by Bhav and while the early lectures seemed to confuse her at times along with the students, by the end they were clear and more useful given that we hadn\u0026rsquo;t encountered the later concepts in previous courses.\nEvery student can tell you about primary and foreign keys in databases, and ER diagrams for different forms of relationships between information. That said, I can\u0026rsquo;t help but feel like this class still left lots to be desired. It should have been what data wrangling (DSCI-523) was but with SQL/JSON and it just didn\u0026rsquo;t live up to that standard. An additional issue was having to use SQLite, which is not really a common language in the working world despite it\u0026rsquo;s syntactical similarities to MySQL, the more common language. Again, the reason for this is UBC does not have an infrastructure in place for relational databases so we were forced to use SQLite. In my opinion, organization would go a long way in improving this class not necessarily changing the instructor or the material itself per se.\nWithout being a huge downer, I still learned lots from this class, and Rodolfo (TA) challenged us in the labs which forced me to get creative with my learning. The majority of my breakthroughs in lab work came through trial-and-error and referencing alternative resources beyond lecture notes to develop the necessary understanding to get through difficult questions. Having to go that extra mile elevated my thinking around how to work with databases, the best ways to design them given particular constraints, and I improved drastically at visualizing different kinds of joins between separate tables! So I guess I have that going for me. In sum, I hope for next year they find ways to improve upon this course structurally, and increase the communication between core faculty and the course instructor regarding what concepts we may have already covered. I still was able to learn a ton about databases, it just took more energy given the constraints I\u0026rsquo;ve mentioned but if you\u0026rsquo;re motivated, you\u0026rsquo;ll be just fine.\nClosing Thoughts \u0026amp; Suggestions Block 3 was by far the most action packed block regarding the density of material and the most exciting given the power of what we were learning. Generally speaking I have great things to say about the teaching staff in this block, I know far more than I did a month ago and don\u0026rsquo;t see myself forgetting the core materials in each class because they were so fundamental. Linear models, machine learning algorithms for classification and regression, how relational databases work via keys and constraints, and script automation are such valuable bodies of knowledge for a data scientist, analyst or really anyone working with data. Aside from some structural issues with one course and the recurring fact that I just don\u0026rsquo;t have enough time to go into everything that intrigues me, this was a great 4 weeks.\nAdditional Ideas/Recommendations One idea that came to me while working in this block is for the wrangling and database classes or really any class that requires more technical skill is to provide a \u0026lsquo;puzzle dataset\u0026rsquo; to students at the start of the block that has various manipulations and is considered \u0026lsquo;untidy\u0026rsquo;. As students learn the skills for a course (perhaps multiple courses) they will be able to apply it to this puzzle dataset and at the end possess all the skills to actually solve it. They can then reference a key that the TA has, and it provides some optional work for students to get practice and tackle optional problems. Obvious classes where this can be implemented are 523 with R (dplyr) and 513 (SQL) but any course where instructors feel they can offer a big picture assignment that students can tackle might be a nice addition to the regular lab work. This could be extended to questions where a dataset must be wrangled and a linear model must be fitted to some portion of the data once it is tidied, incorporating multiple classes and areas of understanding to resolve the problem. It also comes at very little cost to the TA\u0026rsquo;s because the head TA is designing databases to be worked with already by the students, and provides one more large project the students can grade themselves by comparing with a key.\n","date":1545727981,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545727981,"objectID":"14af76675ac9a2acf1f0eea4aba33427","permalink":"https://ehhope.github.io/post/mds-block-3/","publishdate":"2018-12-25T01:53:01-07:00","relpermalink":"/post/mds-block-3/","section":"post","summary":"My experience and thoughts on block 3 of MDS at UBC. The courses were: DSCI 513, DSCI 522, DSCI 561, and DSCI 571.\n","tags":["Machine Learning","Regression","SQL","Script Automation","Docker","MDS","UBC"],"title":"MDS Block 3","type":"post"},{"authors":["Alex Hope"],"categories":[],"content":"My experience and thoughts on block 2 of MDS at UBC. The courses were: DSCI 512, DSCI 523, DSCI 531, and DSCI 552.\nIntroduction to Block 2 I want to break down my thoughts for this block course by course because I have a lot to say, and it doesn\u0026rsquo;t neatly cut across topics, not to mention, my experiences with courses in this block were quite different. First, a few general thoughts before I dive in. I got the most out of courses that I invested myself in, plain and simple. While that seems like a common trope, it really holds true in data wrangling, and algorithms \u0026amp; data structures where the degree of difficulty is quite high in the content, and the lecturer is forced to feed you complex material quickly given the time constraints. The content in a couple courses was quite difficult, and I found myself scrambling two of the four weeks to get my labs in despite putting a ton of effort into my labs. Learning these skills and forms of thinking requires considerable persistence, and for that reason I was ok with barely finishing. This material is supposed to be difficult! That\u0026rsquo;s the point! So with that expressed let me talk about each class.\nAlgorithms and Data Structures (DSCI-512: Patrice Belleville) This was undoubtedly the most difficult course of the block, and the concepts were at times challenging to grasp. The reason they were hard to follow was primarily because of the speed of the curriculum where we would cover hash tables in 25% of one lecture, and touch on different forms of search trees for the rest of lecture before diving into a single lecture on various kinds of directed and undirected graphs. The material was complex but still digestible, and Patrice was a strong instructor, but the time constraints placed a significant burden on people\u0026rsquo;s learning where there was a tendency to learn to get through the course rather than learn to absorb the material of the course for future reference. When the material scales up significantly in difficulty, this was a common feeling in students and I certainly felt it at times as well. For perspective on the pace, undergraduates will spend an entire semester or two examining the full range of material we covered in a single month. This is not only a huge challenge for students, but also the instructor to curate the material and emphasize what is only crucial while discarding or grazing briefly on an area.\nIn addition to the pace, there was a huge gap between the lecture material and the questions being asked in labs where the content we covered needed to be implemented but in a way that was far beyond the coding ability of most of the cohort. Given the general class response I knew I wasn\u0026rsquo;t alone in struggling to apply my understanding of recursion to sierpinski\u0026rsquo;s triangle, or image resizing, but I also secretly enjoyed confronting this massive challenge. This was not an easy introduction to algorithms and how they are implemented in data structures, and I need to learn more on these topics after finishing this class to round out my learning given their importance in data science, but I felt it was more of a structural issue with the disconnect between lecture material and lab material than Patrice\u0026rsquo;s poor teaching that really left me with this impression.\nTo excel in this class you need a strong familiarity with object oriented programming, and should possess an aptitude for reading code that is likely more complex than you can write at this stage. I saw this class as a tough challenge that elevated my thinking on the topic, but didn\u0026rsquo;t provide enough scaffolding to really help me understand the mechanics of how every algorithm worked or how they were implemented in different problems in the labs such as with facebook data that was provided in one lab. That said, if you listen in class you will walk away with an understanding of breadth-first vs depth-first search trees and how they are represented in python. And the different forms of searching whether with rote methods or through recursion. In addition, we spent considerable time looking at how different kinds of graphs can represent information about individual entities, as well as the relationships between these entities. In some cases they were bi-directional and others they were uni-directional depending on the flow of information. The big picture here was, if we wanted to model an algorithm after a particular social media site, or scheduling problem, what would we choose? This high-level planning requires you to know the details about how information is exchanged, referenced and what graphs will and will not allow as far as the flow of that data.\nThis class really could\u0026rsquo;ve been more effective if it had students with a higher coding literacy, and I would beg the instructors to consider scaling up the intro coding classes in block 1 to cover OOP in greater depth and have students write functions sooner in DSCI 511 so they can approach difficult questions in labs with greater confidence. Having to learn to code on top of understanding the algorithms can be daunting, and it would serve all parties involved to make this change I think.\nWhile it seems like I have a lot to say that\u0026rsquo;s negative, part of being a student is also embracing challenge and uncertainty. I learned a lot in this class because I felt like I had to in order to do well, and that pushed me to invest more of myself in it than other classes where I felt it was easier to grab the content quickly and implement the knowledge. Here, I had no choice but to study hard, and that\u0026rsquo;s what being a graduate student is all about\u0026hellip;.. digging deeper.\nData Wrangling (DSCI-523: Jenny Bryant) Data wrangling is a fundamental skill in data science, and that is a fact. A significant portion of your time as an analyst will be spent simply preparing a dataset for analysis and deep exploration. Whether it\u0026rsquo;s changing the type of data in columns, creating 2 columns out of information contained in a single column, creating columns that are aggregates of other columns, joining different tables together, or simply grouping variables together to extract meaning when grouped by their factor, data wrangling really matters and it won\u0026rsquo;t take you long to see why.\nI enjoyed this class the most of any course in the block and perhaps to date in this degree. It was difficult and frustrating at times banging my head against a problem that required multiple joins or a function I wasn\u0026rsquo;t used to applying like lag. Let\u0026rsquo;s just say I had to take many breaks from the computer to finish these labs but out of this immense frustration was an extreme sense of reward when I would learn to parse data in that column, or adjust the time with respect to timezone, or just manipulate data how I intended. I felt powerful, and these are pivotal skills you need to know.\nHaving no experience in R prior to this degree, I couldn\u0026rsquo;t imagine a smoother experience wrangling than with the Tidyverse. It\u0026rsquo;s reputation exists for a reason, it\u0026rsquo;s straight forward and sensical to beginner wranglers and our instructor (Jenny Bryant) is the maintainer of this entire package. Most of all, I was really learning a tangible skill that is readily applicable to personal projects and were employable skills. Along with visualization, it instilled a feeling that I could start to hit the ground running on projects and make reasonable progress working with any data I encountered. The course also provided immediate feedback to me on my abilities and I found that to be quite reinforcing in a way that a class like probability or algorithms was not.\nAs far as Jenny Bryant, her reputation also speaks for itself. She is a world class instructor with quality lecture materials, resources and a great sense of humor that kept me connected to the class and her teaching. I know the cohort genuinely found her lectures to be intriguing, helpful, and balanced in their difficulty as well, introducing us gradually to more and more complex wrangling techniques. It was a pleasure to have her for this class and reassuring that someone heavily involved in the maintenance of the Tidyverse was teaching us how to use it.\nYou may be thinking well wait\u0026hellip;. what about Python? We covered Python\u0026rsquo;s Numpy and Pandas in one class taught by Mike Gelbart, which in my opinion deserved a second lecture given how much Python will be used in ML blocks coming up but mainly focused on R in this class. I believe the thinking was that we can always wrangle in R prior to analyzing in python and while that is fair, python is such a prominent language it would be nice to attend to it\u0026rsquo;s wrangling libraries as well. Admittedly, I didn\u0026rsquo;t really learn anything new in this lecture simply because I knew the Pandas library quite well already. Nonetheless, I am much more confident in my wrangling abilities and genuinely enjoy the challenge of reshaping datasets.\nFor anyone taking this class, you will be well acquainted with how to join different datasets based on particular keys and the type of join you want, how to select particular information in a dataframe, what a tibble is compared to a dataframe, how to add columns, delete data and replace NA\u0026rsquo;s. All of these things are important skills to possess, and I felt for the month we spent wrangling, we covered this material very well. I banged my head on the practice problems enough that the material was seamlessly drilled into my head, and it was so worth it. These are skills anybody working with data will continue to use in the future.\nVisualization I (DSCI 531: Vincenzo Coia) In general, I thought Vincenzo did a great job with this class. He took us through basic graph theory as far as what various plots convey in their variables and information, but also gradually taught us the relevant syntax for ggplot, which has become a personal favourite for visualizing data. Also, graphing data to gather a sense of patterns is under appreciated and not talked about enough. A common mistake is to want to start analyzing immediately and rely on visualizations for the final results to convey information, but Vincenzo really taught me a lot about patience and understanding data through graphs PRIOR to starting any analyses. Get to know what you are working with before investing your time with analysis! Similar to wrangling, we were only introduced to Matplotlib (Vincenzo) in one lecture and Seaborn (Chris- TA) in another lecture. I got a lot out of the Seaborn lecture from Chris, and was relieved to see visualizations in Python can be as easy as in R, but would have loved to see a bit more with respect to python here to round out the class.\nOverall, Vincenzo is a fantastic post-doc who is extremely knowledgeable in R and can convey the basics of graphing extremely well. The only recommendation I have for him as an instructor is to consider a lecture through the lens of a new analyst that wants to understand relationships in the data they have. How can they go about this graphically? What should they look for in the data, what plots should they try, how many different graphs should they plot to get a sense of any patterns? Some form of exploratory thinking would go a long way in adding substance to the syntax we learned and provide much needed direction to students who lack this internal compass. Other than that though he was a good lecturer and whether you have the intuition to explore data or not, anyone who took this class can write the syntax to accomplish what they need.\nYou\u0026rsquo;ll know what kinds of information bar graphs, scatterplots, pie graphs, box plots, word clouds, heatmaps, and map graphs represent and what variables are required to create these visualizations. Also, you\u0026rsquo;ll learn about the ingredients of graphs, so what is the x and y-axis? Are the scales on a log, or normal axis? Can you facet by a certain group to convey multiple plots within a single graph that are separated by group? These sorts of considerations are all touched on intelligently in lecture, and you\u0026rsquo;ll feel comfortable graphing anything basic that you need by the end. Looking forward to seeing what Data Visualization II has in store for us.\nStatistical Inference I (DSCI 552: Tiffany Timbers) First, Tiffany teaches clearly and crisply with great lecture examples that offer insight into her labs directly, so attending her lectures is informative for the work you\u0026rsquo;ll be doing outside of them. She\u0026rsquo;s also a dynamic instructor that educates with class coding, group discussion, and through classic speech and it definitely helped with holding my attention on a topic that doesn\u0026rsquo;t exactly grab most people\u0026rsquo;s focus at first glance.\nBootstrapping is a powerful method for inferring about the meaning of a sample by generating thousands of samples and comparing your original sample test statistic with ones generated from other samples that were bootstrapped. When you do this you get a distribution of test statistics that resembles a normal distribution. This is how you can begin to compare whether there is anything special with your sample in comparison to the many others you\u0026rsquo;ve generated.\nWhile the relevant test statistic you are calculating and assumptions made with a t-distribution (CLT) was nothing new, I still learned the syntax for t-tests, confidence intervals and p-values through R\u0026rsquo;s infer package. It was nice to run over basic stats with Tiffany, and learn about applying bootstrapping as a technique in R, but I also recognize the need for more practice to hammer in these concepts on real live datasets where I\u0026rsquo;m generating my own null/alternative hypotheses and running t-tests and anova\u0026rsquo;s. R is such a powerful statistical language and can do so much of the heavy lifting for you, just ensure you know what you\u0026rsquo;re entering for inputs and how to read the output of your functions.\nAny student that finished this class can tell you what a p-value is and what it signifies with respect to a z-score or t-value in the context of a normal distribution, and what assumptions about normality and standard error exist. Overall, this was a solid class that was a helpful review and introduced us to how R can make our lives infinitely easier once we understand the theory.\nClosing Thoughts on Block 2 My overall impression was positive here, there were far more great moments where I felt absorbed by what I was learning than awful ones where I\u0026rsquo;m overwhelmed but I\u0026rsquo;d be lying if I said this was an easy block. The feeling of drowning in information is part of diving into a new area, let alone one that is extremely complex, new as a field, and constantly evolving like data science. Block 2 definitely felt like a deeper dive into new territory and you can tell the material has started to take an upward incline in difficulty but I felt like I rose to the occasion and as the difficulty increases you can really start to see the value in what you\u0026rsquo;re learning and how it can be applied. This is itself quite motivating, and definitely helps you see the finish line as far as the skills you\u0026rsquo;ll possess and will be required of you beyond this degree. I grew so much with R in this block, despite having serious doubts about it as a language that I would enjoy using in my work. Now, infer, ggplot, and dplyr have completely changed my mind and I will happily use the Tidyverse for future work.\nAt a broader level, the terrain we covered feels like the elementary, foundational skills for data science, and looking at the block 3 course curriculum I\u0026rsquo;ll be curious to see how things progress to the higher-level concepts with regression and machine learning.\nOne major piece that continues to stick with me is the timing of this degree. If this degree was even 6 weeks longer and the curriculum was the same, I think everything would feel a bit less rushed and I\u0026rsquo;d have the time to absorb aa bit more deeply. Don\u0026rsquo;t get me wrong, I am learning a ton, but anytime you try and integrate as much info as this degree demands, you are bound to forget things along the way if it is not used. It\u0026rsquo;s an inevitable downside of the human brain, but that said I\u0026rsquo;m glad I\u0026rsquo;ve squeezed what I could and as it stands I know the material extremely well and will continue to as long as I use it.\nWhat a month it was and onto the next block we go! Until next time\u0026hellip;.\n","date":1545727404,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545727404,"objectID":"72a2d3319581ac103a56d2ace808162a","permalink":"https://ehhope.github.io/post/mds-block-2/","publishdate":"2018-12-25T01:43:24-07:00","relpermalink":"/post/mds-block-2/","section":"post","summary":"My experience and thoughts on block 2 of MDS at UBC. The courses were: DSCI 512, DSCI 523, DSCI 531, and DSCI 552.\n","tags":["UBC","MDS","Data Science","Grad School"],"title":"MDS Block 2","type":"post"},{"authors":null,"categories":null,"content":" Decision Tree Analysis of Makes \u0026amp; Misses in the NBA A case study: Lebron James This project focuses on the 2014-2015 NBA season with a dataset containing characteristics of every shot (shot distance, defender, shot clock, etc.) taken by every player in the NBA. The approach used a decision tree classifier to predict whether a shot was made or missed based on shot features for each shot taken by the player. Lebron James season with the Cleveland Cavaliers was used as a case example, however, the script is flexible enough to wrangle, visualize with several plots, analyze and report data for any player if specified as an argument when using make.\nOur decision tree classifier predicted Lebron James at a marginal ~65% when hyperparameters were optimized, however, other players we were able to score upwards of 85%. The reason for the model\u0026rsquo;s poor accuracy with Lebron James, Steph Curry, and other top shooters is that the features don\u0026rsquo;t segregate makes/misses as cleanly as players who are reliably poor shooters based on changes in particular features such as shot distance. An NBA center will struggle to shoot long range shots, therefore when shot distance increases, our classifier is quite accurate in predicting the outcome.\nThis project is also reproducible through docker. Inside the repository linked below, you will find a dockerfile with instructions for running all of the scripts with their respective dependencies (R and python libraries). If you don\u0026rsquo;t have all of the dependencies installed locally that\u0026rsquo;s ok, just use the dockerfile written to run the scripts in the container provided.\nFor easily reproducing and generating data regarding NBA players, this project uses make to run multiple scripts in terminal as well. If interested, you can find the repository on my github account here with instructions in the README file for using these tools.\n","date":1545553574,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545553574,"objectID":"64ac8478f247230d616bd2af4a89e951","permalink":"https://ehhope.github.io/project/nba-shot-decision-tree/","publishdate":"2018-12-23T01:26:14-07:00","relpermalink":"/project/nba-shot-decision-tree/","section":"project","summary":"A shot analysis of NBA players using a decision tree classifier of makes/misses from the 2014-15 season.","tags":["supervised learning","NBA","Lebron James","decision tree"],"title":"NBA Shot Classifier","type":"project"},{"authors":null,"categories":null,"content":"","date":1540289712,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540289712,"objectID":"b8b5fbbf5d97e48f6720e5344068cac1","permalink":"https://ehhope.github.io/project/brief-intervention/","publishdate":"2018-10-23T03:15:12-07:00","relpermalink":"/project/brief-intervention/","section":"project","summary":"An ecological assessment tool I developed to track anxiety \u0026 depression symptoms \u0026 introduce mindfulness-based techniques to patients enrolled in a BRIEF clinical study.","tags":["intervention","ecological assessments","e-health"],"title":"BRIEF Intervention","type":"project"},{"authors":[],"categories":[],"content":"My experience and thoughts on block 1 of MDS at UBC. The courses were: DSCI 511, DSCI 521, DSCI 542, and DSCI 551.\nHey! My name is Alex Hope, if you\u0026rsquo;ve made it this far I\u0026rsquo;ll spare you the full introduction, however, this post is the first of an ongoing series I\u0026rsquo;ll be doing as I progress through my Master\u0026rsquo;s degree in Data Science (MDS) at the University of British Columbia (UBC). There\u0026rsquo;s a bunch of reasons why I decided to write this blog, but a major one was to track my own growth as I transition into this field from healthcare research so I can look back on how far I\u0026rsquo;ve come and recall concepts from the coursework of MDS in the process. I\u0026rsquo;m excited to dive into this new degree and learn more about the analytical and computational skills I knew were important but never had the time to cultivate while working. In addition, I wanted to use this blog as a resource for future students in MDS who like myself may have been unsure about what to expect as a student in MDS, so I hope any of this may be helpful. This blog is a perspective from someone who is simply going through the program as a student, not as a vested employee of UBC. So, you\u0026rsquo;ll hear my thoughts and advice about courses and instructors and I will do my best to convey my feelings about each class as honestly as I can. First a bit of background about what I did prior to getting here.\nThe resources I used to prepare for this degree: * Work experience writing code to do basic analysis in Python and building a basic site with HTML/CSS * Jose Portilla\u0026rsquo;s: Python for Data Science, and Complete Python Programming Bootcamp * ProjectEuler.com - bunch of coding problems on there * Probability and Statistics materials from a one day trial of a bootcamp called \u0026ldquo;Metis\u0026rdquo; * DataCamp: 4 courses (python and basic statistics)\nBlock 1 With that in mind lets kick this off! My cohort contained about 70 students with people of all ages, genders, and countries with diverse skill sets. We have former business owners, rehabilitative therapists, engineers, mathematicians, fellow neuroscientists, programmers, even a practicing doctor. As far as the coursework, there are 6 blocks in MDS with each lasting ~ 1 month. Each block contains 4 classes, the first four being: DSCI 511 (Intro to Programming), DSCI 521 (Computing Platforms for Data Science), DSCI 542 (Communication \u0026amp; Argumentation), \u0026amp; DSCI 551 (Descriptive Statistics \u0026amp; Probability). Essentially, every class has a lab due on the Saturday (4 labs due each week), with midterms taking place during the 2nd week of the block and then finals being the 4th week of the block. So it\u0026rsquo;s pretty fast-paced. Without further a due, let\u0026rsquo;s dive in.\nIntroduction to Programming (DSCI 511) This class was an interesting split between R and Python, with separate instructors that taught us for each language. Heading into the fall I knew I needed to sharpen my programming skills to be able to hit the ground running in this degree and the last thing I wanted was to fall behind early on. Having used Python in my past work, I wasn\u0026rsquo;t worried about keeping up for this part of the class but was concerned about the fact I had zero experience using R. I had heard that R was this language of the past and that it would be overtaken by languages like Python and Julia eventually, so I chose to focus on Python than splitting my learning between languages. In truth, I loved learning about R, and found the in-class tutorials by Vincenzo Coia to be extremely helpful and easy to grasp. Now that I\u0026rsquo;ve used R I feel like it\u0026rsquo;s arguably an even easier language to learn and use immediately than Python, and can see why it\u0026rsquo;s recommended as a great starting language. On the python side, I would say I over-prepared for this class, I wasn\u0026rsquo;t challenged much to be honest. If I had looked at even a little R I think I would\u0026rsquo;ve been fine getting through this block 1 class. I enjoyed the in-class programming tutorials that linked the ideas they were trying to convey with motor-learning, it always helps to write the code yourself and follow along I find, and both instructors took their time conveying the material while providing notes for you to refer back to if needed.\nIn general, the class lectures were well paced with more difficult labs than exams, and they truly distill the basics of what is required to get comfortable writing functions, understanding object-oriented programming (OOP), lists, vectors, dictionaries, and using different kinds of loops, which I trust we will be deploying on an everyday basis in future blocks. There were students that barely used either language or had zero programming experience heading into the fall who did fine, so while the requirements suggest you need to have all of these mathematical, statistical and programming skills to be ready for the degree, I wouldn\u0026rsquo;t say it\u0026rsquo;s absolutely necessary, but it definitely won\u0026rsquo;t hurt. I\u0026rsquo;ll be curious to see how they emphasize these languages differently as we progress to different parts of the data science process (graphs, analysis, reports, slides, etc.) where perhaps having a greater proficiency will help. As far as getting through this class though and performing well, you don\u0026rsquo;t need much of a background honestly.\nComputing Platforms in Data Science (DSCI 521) I wasn\u0026rsquo;t quite sure what to expect from this class heading in as I had never really used any platforms aside from Jupyter that were for data science per se (SPSS and Tableau aren\u0026rsquo;t really considered DS platforms in my opinion). Tiffany Timbers taught this class and introduced us to the beauty of GitHub, RStudio (Markdown and RMarkdown) and Jupyter Notebooks (RISE).\nGitHub is a well-known service that allows people to share all kinds of projects, tools, code, webpages, among other things seamlessly to the web and between people. It\u0026rsquo;s a big part of advertising your own work as well as collaborating with others on things so I\u0026rsquo;m glad we focused on it because it\u0026rsquo;s a great way to gain traction with employers to show them what you can do. On a practical note, we will be using it across all of our classes throughout the degree to pull labs and quizzes from the cloud to our local machine and I suspect we will be cloning, adding, committing, pulling, pushing and forking a lot over the next year\u0026hellip;\nHaving learned about RStudio in DSCI 511, it wasn\u0026rsquo;t completely foreign to me, but there is lots to learn that you simply can\u0026rsquo;t cover in one class as well, so I\u0026rsquo;m glad we covered it in two classes. One aspect I did learn was how to change the output of md and Rmd files to knit different sorts of files into slides, pdf\u0026rsquo;s, or any other useful file imaginable! In addition to RStudio is Jupyter Notebook, which is a complimentary tool used to write code, as well as write documents and will be a staple within the program\u0026rsquo;s learning technologies. I had used Jupyter in the past while learning Python so I was familiar with how it worked in some ways, but I was unaware of RISE which is a way to use notebooks as a powerpoint presentation but with additional features that allow you to have interactive code embedded in your presentations. Very cool.\nIn general, this DS stack seems useful across personal and business settings and I could see myself continuing to use all of these in the future for presentations, writing, sharing documents and coding. Tiffany was great at conveying the basics, showing us directly how they work without overwhelming students and providing resources to take our learning even further beyond lecture if they wanted. She is also extremely responsive on slack, even at odd hours of the day when you think she would be sleeping or doing other human things on a Sunday, she will often respond. Her commitment to students in this class was pretty hard to miss.\nI didn\u0026rsquo;t find the material overly challenging, and really you will learn to use these technologies simply through sheer practice because you will lean on them heavily across almost all of your classes in this block and future ones. Tiffany also demonstrated how to host your own website through GitHub Pages (which you can use to show off your MDS work), how to fork other people\u0026rsquo;s repositories that provide the framework for writing a book, and how to post issues and comments to other repositories as a means of helping correct for bugs. There is so much to explore within GitHub.\nOne day I will write that bio technology book\u0026hellip; one day. No time right now in MDS\u0026hellip; but one day.\nCommunication and Argumentation (DSCI 542) David Laing (a former MDS student) was the instructor for this class, which was the only one of it\u0026rsquo;s kind that you wouldn\u0026rsquo;t consider \u0026lsquo;technical\u0026rsquo; and I have to say I really enjoyed the material. The class focused on \u0026lsquo;softer skills\u0026rsquo; in data science around how to frame problems, communicate the meaning of statistical results and build awareness for how complicated it can be to communicate ideas to different audiences, and I thought David\u0026rsquo;s strong organization and focus on communicative clarity in lectures helped with digesting the material and got you thinking. He also provided ample time to discuss with classmates problems he had framed, and asked us to participate in daily exercises that incorporated the material he was teaching. Overall, he did a great job.\nA common saying hammered into me from this class that was handed down from Hadley Wickham - \u0026ldquo;it doesn\u0026rsquo;t matter how good your analysis is unless you can explain it to others: you need to communicate your results\u0026rdquo;. Start to finish communication is an important aspect of data science and the more I progressed through this class the more I realized the truth behind this fact. From the beginning to the end of a data science project, communication is an important component and can save enormous amounts of time and energy when done well, but haunt you for hours/days/weeks on end when done poorly. Writing code and planning the overarching pieces of an individual project can be challenging because you aren\u0026rsquo;t always clear with yourself on what\u0026rsquo;s required let alone when you have to work with others on a large project.\nWe spent time talking about the barriers to communication, mainly, struggling to imagine how to explain a concept to someone who doesn\u0026rsquo;t understand data science or does but at a different level than you. This includes the tendency to use jargon you comprehend that others don\u0026rsquo;t, employ complex examples and make assumptions in your writing that may need to be spelled out. Understanding the literacy of your audience and how to present information in a way that makes it easy for people to digest is a crucial skill that I\u0026rsquo;m glad we could learn to harness here. One concept was to use bottom-up perspectives where examples are presented to convey the critical point of your message in a simple way before elaborating further on the topic once a case has been presented to the listener.\nIn the end, each student was asked to write a blog post on any topic they wanted in Data Science that attempted to capitalize on the techniques we had learned for presenting information clearly. I thought it was an excellent exercise that summarized the course well, and gave us something to contribute to our webpage that we had developed in DSCI 521 (you can see how these courses start to gel together). At the end of the course, we gave a presentation on our blog post or a data science problem we had formulated from a later lab. I thought the public component really forced students to integrate the knowledge from lecture, and you could tell MDS was organized because they recommended that we use RStudio or RISE to present our topic to the class to further consolidate what we were learning.\nI really enjoyed this class and it was a nice twist from the technical classes we had been focused on. I thought David was fantastic and hope he continues to teach as you can tell her clearly enjoys it. At worst David got you thinking about the challenges of explaining data science, and at best he changed your thinking, writing and speaking so that you delivered content in a more conscientious way to the end consumer, whoever that may be.\nDescriptive Statistics and Probability (DSCI 551) This was probably the one class that you would like to have had some background in coming into MDS block 1. As someone who always struggled with statistics in undergrad, it would\u0026rsquo;ve been nice to look at some basic stats \u0026amp; probability a bit more than I did. The class isn\u0026rsquo;t hard if you invest yourself in it and listen closely in lecture, but make no mistake it was the toughest class of block 1 for English speaking students (DSCI 542: Communication was supposedly tougher for many ESL students).\nMike really focused on descriptive stats (variance, standard deviation, entropy, mean, median, and mode), probability (conditional, joint, and marginal distributions) and examining what different kinds of distributions look like using graphs. As a data scientist, probability is a fundamental knowledge base for all sorts of ML and statistical procedures (Ex. Bayesian Theory) and I knew this was pivotal for what we would learn moving forward in MDS.\nI thought Mike Gelbart (professor) did a pretty good job educating us on how distributions work (Bernoulli, Gaussian, Multinomial), and showing us how R can help derive different measures of distributions using it\u0026rsquo;s functionality as well as emphasizing theory. He is also extremely responsive in the slack channel if you have any questions or hit a snag. Similar to all of the professors, the notes were in Jupyter notebooks and I found playing around with the interactive notebooks for this class to be helpful for learning about the behavior of distributions. Mike is very good at explaining concepts simply, and presenting it with multiple perspectives which I appreciated since this class was by far the least intuitive to me. I left with a strong grasp of parameters for distributions, visualizing conditional and joint distributions, correlations in graphs, and how differences in variance and entropy can effect the shape of distributions when graphed. I had to work the hardest in this class but like anything, you get what you invest into these things.\nTo conclude, this was a great first block and the organization between courses in how they integrate is unparalleled. It\u0026rsquo;s a huge help to have people curate material in a way that\u0026rsquo;s intelligent and efficient, and you truly do get the impression this has been well thought out since the program\u0026rsquo;s first inception. The effort MDS faculty have put into designing the program does offload some of the burden of having to \u0026lsquo;know all the prerequisites\u0026rsquo; before you start your degree despite MDS being a fast-paced learning environment, make no mistake.\nIn my opinion, students who feel they aren\u0026rsquo;t prepared will most likely be just fine for this intro block. Of course, the more you know going in the more free time you have to focus on side projects and dig deeper into material you know the basics of already so there is that. I can tell there\u0026rsquo;s a lot of talented, driven people to work with this coming year, which is exciting.\nOne last point that I think is important is the program encourages us to learn as much as we can by specifically NOT breeding a sense of competition amongst peers. You can see this in how they have us interact during lecture, troubleshoot each other\u0026rsquo;s problems on MDS\u0026rsquo; slack, and emphasize that our classes are not curved. It helps breed an awesome collaborative spirit within the cohort that will resonate with most students\u0026rsquo; careers beyond MDS once they graduate. Also, one class starting in block 3 until block 6 will supposedly have a partner component where you work together on a project. So, with all that said, can\u0026rsquo;t wait for what block 2 has in store.\n","date":1539227181,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539227181,"objectID":"a1fe83781d5c725ea5dbf3b06d6db6b9","permalink":"https://ehhope.github.io/post/mds-block-1/","publishdate":"2018-10-10T20:06:21-07:00","relpermalink":"/post/mds-block-1/","section":"post","summary":"My experience and thoughts on block 1 of MDS at UBC. The courses were: DSCI 511, DSCI 521, DSCI 542, and DSCI 551.\n","tags":[],"title":"MDS Block 1","type":"post"},{"authors":["Alex Hope"],"categories":[],"content":"This post is on Overfitting, what it is and why it matters.\nOverfitting What is overfitting exactly? Why does it matter?\nThe truth is the word has a few definitions, and depending on who you ask and the context in which the word is used you may receive a wildly different answer. When I talk about overfitting I am not referring to when someone has outgrown their old pants or when a pipe is too large for it\u0026rsquo;s receiver. I\u0026rsquo;m talking about it\u0026rsquo;s use in statistics. Here, overfitting is a common pitfall in data analysis where a particular model explains observations in a dataset too precisely and confidently.\nWait though, wouldn\u0026rsquo;t explaining data accurately and with confidence be a good thing?\nWell yes, accurately detailing trends in a dataset is considered a good thing but if that explanation isn\u0026rsquo;t useful when applying it to future datasets than it defeats the purpose of why we are even here. To predict the future! Just because a model perfectly explains the information in a given dataset does not mean it is a) a great model and b) suitable to explain other observations it has yet to encounter. One sign that your model may be overfitted to your data is when there is very low variance that it fails to account for. In fact, if a model has been trained to predict patterns of data in an original \u0026lsquo;training dataset\u0026rsquo; too strongly, it will struggle with making close approximations about patterns in new datasets that vary in small or large ways that may or may not be interesting (natural variation vs error).\nOf course, we want to build a model that accurately accounts for both present and future observations, however, if the model is overfitted to the present data than it will struggle with predicting new information that do not fit cleanly into the distribution of the original dataset it was trained on. This is a common issue and there are many ways to combat this problem, however, that goes beyond the scope of this topic.\nThe fact of the matter is data is very messy, and will not arrive in identically similar ways because measuring phenomena in the world is a complex and difficult endeavor. A delicate balance always needs to be struck between highlighting interesting variation in a dataset and ignoring aspects that are simply noise, and mark my words, noise will almost always exist.\nThat was a lot of words, let\u0026rsquo;s break it down visually. What are examples of different kinds of fittings? Below you will find examples of a) underfitting, b) appropriate fitting, and c) overfitting that outline how different quadratic functions of models can be fitted to data. Bear in mind, not all models are made equal!\nHere are examples of different \u0026ldquo;fits\u0026rdquo; on the same data:\nThe images above represent three examples of different functions that capture trends in a set of observations (i.e \u0026lsquo;X\u0026rsquo; and \u0026lsquo;O\u0026rsquo;). In each of the above pictures the aim of the model is to best capture the position of \u0026lsquo;O\u0026rsquo; and \u0026lsquo;X\u0026rsquo; datapoints as they are distributed across the x- and y-axis. However, some capture the data better than others. The far right image is an example of overfitting where the function has been forced to fit all of the \u0026lsquo;O\u0026rsquo; observations and all of the \u0026lsquo;X\u0026rsquo; observations. As you can see it captures it perfectly! But how do you think it would hold up if a new dataset was presented on the same 2-plane axis? Probably not very well since observations tend to move at least slightly within a distribution and if any of these data points were to move in the axes, the model would hold much less predictive power. This is what is meant by picking a suitable model over a perfect model. We are interested in generalizing our predictions to other phenomena, not perfectly modeling observations that already exist, anyone can do that.\nConversely, on the left image you have under-fitting where the model hasn\u0026rsquo;t been trained adequately to grasp patterns in a dataset that are clearly meaningful signals. In those cases the predictive power of the model will be impaired as well, and so the delicate balance between both extremes leads to an appropriate fit shown in the middle. Here, you can see that the appropriate fit does not capture the data perfectly, however, it does do an excellent job at explaining the general direction of the trend while leaving enough room for observations to potentially fall on either side of the quadratic function. Essentially, this model predicts the trend well but also leaves room for error within the distribution, which will almost always exist.\nWhy does this topic even matter?\nThis topic matters a ton! In the current data-driven world we inhabit, humans are increasingly relying on data to aid their decision-making across both their personal and professional lives. Making an informed decision about anything hinges on the quality of the information being received, and the estimations made based on that information. We do this by building effective, properly fitted models that make accurate predictions about the future. From marketing to production lines to brain-imaging to product design and consumption, analytics play a role across many facets of society. My hope is that you now understand what the term overfitting refers to in statistics, the consequences of overfitting statistical models and what the balance is when fitting a model to data.\n","date":1537076486,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537076486,"objectID":"23dad439a26b14f42602392bd4839f21","permalink":"https://ehhope.github.io/post/overfitting/","publishdate":"2018-09-15T22:41:26-07:00","relpermalink":"/post/overfitting/","section":"post","summary":"This post is on Overfitting, what it is and why it matters.\n","tags":[],"title":"Overfitting","type":"post"},{"authors":["Alvarez, L., Wiebe, S.A., Adams, K., Hope, A., \u0026 Cook, A."],"categories":null,"content":"The strong relationship between motor and cognitive development suggests that the limited motor experience of children with physical disabilities can impact their cognitive and perceptual development. The assessment of their cognitive skills is also compromised due to limited verbal communication and motor gestures. Robots have been used to give children with disabilities an opportunity to independently manipulate objects and to reveal their cognitive skills when they use the robots. Little is known about the neural correlates that subtend robotic augmentative manipulation and the ways in which using a robot to manipulate objects may change the task\u0026rsquo;s cognitive and perceptual demands. Several technical considerations pose a challenge to such studies.\nOBJECTIVE: This paper presents a methodology for the technical implementation of neurophysiological exploration of robot-augmented manipulation and presents an evaluation of the technical feasibility of performing a comparison between augmented manipulation and direct manipulation as response modalities in a cognitive task.\nMETHODS: A costume made interface was designed that would allow the interfacing of the EGI NetStation Electroencephalographic (EEG) signal acquisition system, the E-Prime stimulus presentation system, and a 3-Dimensional task performed with either a robot or through typical direct manipulation. The technical feasibility and the stability of the designed technical implementation was tested with 10 adult participants.\nRESULTS: Initial analysis revealed specific robot control interface related artifacts. Further testing confirmed the source of artifact. Independent Component Analysis (ICA) was successfully used to separate this artifact component. Advantages, disadvantages, and results obtained from this method for technical implementation are presented. Implications for the study of neural correlates of augmentative manipulation are discussed.\n","date":1396403478,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396403478,"objectID":"4a1eb81794032ac0880bf683e8190754","permalink":"https://ehhope.github.io/publication/neurophysiology/","publishdate":"2014-04-01T18:51:18-07:00","relpermalink":"/publication/neurophysiology/","section":"publication","summary":"The strong relationship between motor and cognitive development suggests that the limited motor experience of children with physical disabilities can impact their cognitive and perceptual development. The assessment of their cognitive skills is also compromised due to limited verbal communication and motor gestures. Robots have been used to give children with disabilities an opportunity to independently manipulate objects and to reveal their cognitive skills when they use the robots. Little is known about the neural correlates that subtend robotic augmentative manipulation and the ways in which using a robot to manipulate objects may change the task\u0026rsquo;s cognitive and perceptual demands.","tags":[],"title":"The Neurophysiology of Motor Manipulation","type":"publication"}]